\documentclass[11pt]{article}
\input{preamble.tex}
\input{preamble_mgc.tex}
\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}
\title{\vspace{-2em}\bf Discovering Relationships Across Disparate Data Modalities}

\author[1,2]{Cencheng Shen} %\thanks{cshen6@jhu.edu}}
\author[1,3]{Carey E. Priebe}% \thanks{cep@jhu.edu}}
\author[3,4,6]{Mauro Maggioni}%\thanks{mauro.maggioni@jhu.edu}}
\author[1,5,6]{Joshua T. Vogelstein\thanks{jovo@jhu.edu}}
\affil[1]{Center for Imaging Science, Johns Hopkins University (JHU)}
\affil[2]{Department of Statistics, Temple University}
\affil[3]{Department of Applied Mathematics and Statistics, JHU}
\affil[4]{Department of Mathematics, JHU}
\affil[5]{Department of Biomedical Engineering and Institute for Computational Medicine, JHU}
\affil[6]{Institute for Data-Intensive Engineering \& Science, JHU}
% \affil[7]{Institute for Computational Medicine, Johns Hopkins University}
\maketitle 
\thispagestyle{empty}
\date{}
% \bigskip
\vspace{-15pt}
\begin{abstract}
% \vspace{-5pt}
Discovering whether certain properties are related to other properties is fundamental to quantitative investigation. As data collection rates accelerate, it is becoming increasingly difficult and important to determine whether one property of data (e.g., cloud density) is related to another (e.g., grass wetness). Only if two properties are related does it make sense to further investigate the nature of the relationship. 
Existing approaches excel in different settings, with no one approach dominating for all relationships and sample sizes, including data in high-dimensions with structure and nonlinear relationships.
We juxtapose three previously discordant methods in data science---hypothesis testing, manifold learning, and harmonic analysis---to obtain an approach we call Multiscale Generalized Correlation (\Mgc). 
% @jovo: maybe not "insight".  
Our key insight is that we can adaptively learn to restrict the analysis to ``jointly local'' observations---that is, observations that are nearest neighbors for both the properties being compared. %---by estimating the optimal locality from the data.  
\Mgc~statistically dominates previous approaches while maintaining computational efficiency.
% @bm: is the above better?
We used \Mgc~to detect the presence and reveal the nature of the relationships between brain properties (including activity, shape, and connectivity) and mental properties (including personality, health, and creativity), while avoiding  the false positive inflation problem that has plagued conventional parametric approaches. 
Our open source implementation of \Mgc~is easy to use and applicable to previously vexing questions confronting science, government, finance, and other disciplines. 
\end{abstract}


\noindent%
{\it Keywords: testing independence, distance correlation, k-nearest-neighbor, kernel test, permutation test}

\clearpage
\setcounter{tocdepth}{2}


Identifying the existence of a relationship is the initial, critical step in the investigation of any properties within a dataset. Only if there is a statistically significant relationship does it make sense to determine whether the relationship has predictive power or whether it reflects causality.
One of the first approaches for determining whether two properties are related to---or statistically dependent on---one another is Pearson's Product-Moment Correlation (published in 1895 \cite{Pearson1895}). This seminal paper prompted the development of  entirely new ways of thinking about and quantifying relationships (see \cite{Reimherr2013,JosseHolmes2013} for  recent reviews and discussion).


Modern datasets, however, present  challenges for dependence-testing that were not addressed in Pearson's era.
%
First, the dependencies between different properties 
of data can be highly {nonlinear}.
% 
Second, the dimensionality of the data might be extremely high (millions or billions for genomics and connectomics datasets, for example), while the sample size often remains low (tens or hundreds).  This ``{large p, small n}'' problem compounds the challenges associated with nonlinear relationships because higher dimensional problems require more data to obtain accurate estimates \cite{johnstone2009statistical}.
Third, the data are often {structured}---sequences, images, networks, shapes, and text---creating problems for standard methods that were developed for unstructured feature sets such as real numbers \cite{bakir2007predicting}.
Fourth, because of the accelerating data deluge,  {computationally efficient} methods are critical for generating results within acceptable time frames \cite{hey2009fourth}.
Modern tests therefore should perform well for any linear or nonlinear relationships, low-dimensional or high-dimensional data, with or without structure, with theoretical guarantees using computationally efficient algorithms, and good empirical performance with low sample size.
And finally, investigators not only want to know {whether}  properties are dependent on each other, but also {how they are related.}
% @bm: what do you think of the below sentence?
These five issues are particularly problematic for our motivating problems: understanding the relationship between brain properties (such as activity, shape, and connectivity) and mental properties (such as personality, disease status, and creativity).



Many statistical and machine learning approaches have been developed over the last 120 years to combat the above issues. Specifically, pairwise comparison-based approaches have been developed for  a wide variety of tasks,  ranging from hypothesis testing (e.g., \cite{David1966,Mantel1967,Friedman1983,Schilling1986,Maa1996,SzekelyRizzo2009,SzekelyRizzo2013b,HellerGorfine2013,Dumcke2014}) 
to manifold learning (e.g., \cite{TorgersonBook, TenenbaumSilvaLangford2000, SaulRoweis2000, BelkinNiyogi2003,DiffusionPNAS, MMS:NoisyDictionaryLearning}); kernel machines are a sub-field of machine learning dedicated to using such comparisons (e.g., \cite{scholkopf2002learning,GrettonEtAl2005,harchaoui2013kernel}).
The initial step of these approaches is the computation of \emph{comparisons} (either  similarities or dissimilarities) between all pairs of observations; typically, based on Euclidean distances (the sum of squared errors).
These methods differ in how they choose the comparison function(s) and what they do with the comparisons after computing them. 
The popularity of these methods can be attributed to their strong theoretical properties \cite{SilvaTenenbaum2002,Allard2012}, wide-ranging applicability to structured data \cite{scholkopf2002learning}, relative computational efficiency, and empirical performance \cite{lu2014scale}. 

% @bm: below is good?
While the above approaches have successfully addressed some of the five aforementioned challenges,  no test has successfully addressed them all.    Hoeffding and Renyi proposed non-parametric tests to address nonlinear but univariate relationships \cite{Hoeffding1948,Renyi1959}.  In the 1970s and 1980s nearest neighbor style approaches were popularized \cite{Stone1977}, including for dependence testing purposes \cite{Friedman1983,Schilling1986}.  These tests discard all but the $k$ nearest neighbors for both properties, and then simply compute the fraction of pairs that are ``jointly neighbors'', that is, pairs that are neighbors in both properties. This requires specifying $k$ in advance, and ignores the magnitude of the correlation by only counting whether pairs are jointly neighbors.  Such local methods are therefore sensitive to choice of $k$, and are uninformative  when $k$ is large (because nearly all pairs are neighbors), resulting in poor empirical properties.
For these reasons, global approaches have prevailed over ``local'' tests in practice.  The more recent Distance CORRelation test (\Dcorr) is asymptotically consistent (meaning will reject when the null is false for large enough sample size) for arbitrary nonlinearities \cite{SzekelyRizzo2009}, for large dimensions \cite{SzekelyRizzo2013a}, and structured data \cite{Lyons2013}.
Around the same time, Heller, Heller, and Gorfine proposed a test (\Hhg), that shared similar properties \cite{HellerGorfine2013}.  Empirically, \Dcorr~performs well in high-dimensional linear data, whereas \Hhg~performs well in low-dimensional nonlinear data,
but neither performs particularly well in high-dimensional nonlinear data, which characterizes a large fraction of real data challenges in our current big data era. And none of these approaches provide insight into the nature of dependence.

We surmised that the reason no method dominated the other existing tools across nonlinearities and dimensionalities was because none of the existing approaches are sufficiently \emph{adaptive} to the data \cite{zhang2012adaptive}.  Specifically, they each rely on an \emph{a priori} selection of an algorithmic parameter, such as the kernel  \cite{scholkopf2002learning}, intrinsic dimension \cite{RoweisSaul2003}, and/or local scale \cite{Friedman1983,Schilling1986,Allard2012}. Indeed, the fragility of manifold learning, its Achilles Heel, has been the requirement to manually choose these parameters \cite{levina2004maximum}.

% @jovo: supplemental table?



To illustrate the importance of adapting to different kinds of relationships, imagine investigating whether there is a relationship between cloud density and grass wetness. If the relationship between cloud density and grass wetness is approximately linear, the data might look like those in Figure \ref{f:newschem}{\color{magenta}A}. 
On the other hand, if the relationship is nonlinear---such as a  spiral---it might look like those in Figure \ref{f:newschem}{\color{magenta}B}.
Although the relationship between clouds and grass is unlikely to be spiral, spiral relationships are prevalent in nature and mathematics, and are canonical in evaluations of manifold learning techniques \cite{Lee07a}, thereby motivating its use here.
In either case, the data consist of $n$ observations of both cloud density and grass wetness under those clouds.
% (Figure \ref{f:newschem}). 
Let $x_i$ denote cloud density for observation $i$ and $y_i$ denote grass wetness on that same observation. 
% @bm: i kept explicit example first, but still re-ordered paragraph to be better.  i think this is better.


\begin{SCfigure}
%  trim={<left> <lower> <right> <upper>}
%\includegraphics[height=0.9\textheight,trim={0cm 1.8cm 0 2cm},clip]{Figures/Fig1Allb.pdf}
\includegraphics[width=0.54\linewidth]{Figures/Fig1Allb.pdf}
\caption{Illustration of the three steps of Multiscale Generalized Correlation (\Mgc)  using  $100$ pairs of cloud density ($x_i$) and grass wetness ($y_i$). 
We present two different settings: \textbf{(A)} linear  and \textbf{(B)} nonlinear (spiral; see Appendix \ref{appen:function} for simulation details). 
Insights into the data available only from running \Mgc~are highlighted in {\color{green}green.}  Results using \Dcorr~\cite{SzekelyRizzo2009}, a state-of-the-art dependence test that \Mgc~extends, are shown for comparative purposes. 
% 
Samples $1$, $2$, and $3$ (black) indicate how \Mgc~is able to discover nonlinear relationships (arrows show $x$ distances and $y$ distances between points 2 and 3). 
% 
The three steps of \Mgc~are:
% 
\textbf{(i)} Compute all distance pairs. Distances are linearly correlated in the linear setting, whereas they are not in the spiral setting.  \Dcorr~uses all distances (gray dots) to compute its test statistic, $\GG(\Dcorr)$, which \Dcorr~uses to compute its p-value.
% 
\textbf{(ii)} Find the maximum local generalized correlation between $x$ distances and $y$ distances.  The scale with maximum local generalized correlation is the global scale for the linear setting, where it is a very local scale for the spiral setting (panel titles state the maximum local generalized correlation $\GG(\Mgc)$, and the scales that achieve it; green circles show the distances included in that scale).
\textbf{(iii)} Compute the p-value and estimate the optimal scales.
The heatmap shows the local p-values for all scales (computed via a permutation test). The green circle indicates the scale with maximum local generalized correlation (from step (ii));  the estimated optimal scales are all scales within the green rectangle, which is the largest rectangle whose elements all have small local significance values. The global scale (gray dot) is always in the top right corner, regardless of the data. 
Titles state the p-values,  $p(\Mgc)$~and $p(\Dcorr)$).
Unlike \Dcorr, \Mgc~is able to detect and reveal the scales of dependence in both linear and highly nonlinear settings, even with low sample sizes.}
\label{f:newschem}
\end{SCfigure}



Now consider observations  1, 2, and 3 (highlighted in black).  Under the linear relationship, when a pair of observations are close to one another in cloud density, they  also tend to be close to one another in grass wetness (for example, observations 1 and 2).
Similarly, 
when a pair of observations are far from one another in cloud density, they also tend to be far from one another in grass wetness (for example, observations 2 and 3).  
This suggests that global correlation between distances---the correlation between all $x$ distances and the corresponding $y$ distances for each pair of observations---can determine whether these two properties are related.  Moreover, it suggests that the more of these distances we use, the more information we will obtain, yielding a more powerful test.
% 
On the other hand, consider the nonlinear (spiral) relationship.  Here again, when a pair of observations are close to one another in cloud density, they also tend to be close to one another in grass wetness (see points 1 and 2 again).  However, in this nonlinear relationship,  a large distance between cloud densities does not necessarily imply that the distance between corresponding grass wetnesses also tends to be large (see points 2 and 3).
Thus, the global correlation between cloud density distances and grass wetness distances is weak, misleadingly suggesting that there is little or no statistical dependency between density and wetness, even though there is one at more local scales.
One could restrict the distances to only consider  the nearest neighbors of each point, but how many nearest neighbors should one use?  Use too few and much of the data are ignored; use too many and nonlinear relationships could be overlooked. 
% @jovo: consider introducing the character of distances of shapes here.
  
The key, therefore, to successfully determining the presence and nature of a relationship is to adaptively estimate the number of neighbors that are particularly informative.
This is especially important in high-dimensional data, where simple visualizations do not reveal the relationships to the unaided human eye.
Our  dependence test---called ``Multiscale Generalized Correlation'' (\Mgc)---extends essentially all previously proposed pairwise comparison-based approaches to enable estimation of the  optimal scales.   
% \Mgc~explicitly addresses the fundamental trade-off suggested above: as more neighbors are included (that is, as the scale increases), the effective sample size increases; however, more neighbors also means that in nonlinear relationships, the neighbors are less likely to be linearly related, potentially decreasing power.
{Our main insight is that we can adaptively estimate informative scales for any relationship---linear or nonlinear, low-dimensional or high-dimensional, unstructured or structured---in a computationally efficient and statistically consistent fashion, therefore guaranteeing as good or better  statistical performance compared to existing global methods for any finite sample size.}
Moreover, the estimated scales are informative about the nature of the dependence structure, therefore providing further guidance for subsequent experimental or analytical steps. \Mgc~is thus a hypothesis-testing methodology that builds on recent developments in manifold learning (operating on pairwise comparisons) by combining them with complementary developments in harmonic (multiscale) analysis. 
It is this union of three disparate disciplines spanning data science that enables improved theoretical and empirical performances. 


The first step of \Mgc~is the same as essentially all other nonparametric dependency tests:
compute the distances between all pairs of cloud densities and the corresponding distances between all pairs of grass wetnesses (Figure \ref{f:newschem}{\color{magenta}Ai} and {\color{magenta}Bi}).
Unlike previously proposed tests that specify the scale \emph{a priori} (potentially implicitly specifying the global scale),  the second step of \Mgc~is to find the maximum ``local generalized correlation''.
A local generalized correlation is the correlation only including the $k$ smallest distances for each $x_i$, and the $l$ smallest distances for each  $y_i$.  \Mgc~computes these local generalized correlations for all possible values of $k$ and $l$, incrementally increasing the number of neighbors for each $x_i$, and separately increasing the number of neighbors for each $y_i$.
The \Mgc~\emph{test statistic} is the local generalized correlation with the best scale, that is, the $(k,l)$ pair whose generalized correlation is largest after smoothing (\Mgc~smooths to address noisy samples). 
The green circles in Figure \ref{f:newschem}{\color{magenta}Aii} and {\color{magenta}Bii} show the set of distances amongst the $(k,l)$ nearest neighbors that \Mgc~selected for these particular simulations.
For the linear case, all the neighbors are used, so  \Mgc's test statistic is the same as \Dcorr's (which uses all neighbors).  For the nonlinear case, however, the set of comparisons is limited to only local pairs, resulting in \Mgc~yielding a larger test statistic  than \Dcorr's (titles show the maximal test statistics and its corresponding local scale).   

The third and final step is to compute the p-value, report the  {multiscale significance map}, and estimate the optimal scales  (Figure \ref{f:newschem}{\color{magenta}Aiii} and {\color{magenta}Biii}). 
The p-value is available via a permutation test.  For each iteration, permute the labels of either the $x_i$'s or the $y_i$'s, and compute the maximum local generalized correlation and its scales (the green circles show the selected scale).  This procedure avoids the multiple hypothesis testing problem entirely by only computing the p-value for the scale with the maximum local generalized correlation, ensuring that \Mgc~is a valid and unbiased test (see Appendix \ref{appen:algorithms} for details).  The procedure also computes ``local significance values''  for each $(k,l)$ scale.  The multiscale significance map is the set of all of these significances.   
The estimated optimal scales (green boxes) are all the scales within the largest rectangle that includes local significance values all smaller than that p-value.
For the linear example, many scales including the global one (\Dcorr's) yield low significance values, implying a nearly linear relationship.
On the other hand, for the nonlinear setting, only a set of small local scales yields low significance values, implying a strong nonlinear relationship that is undetected by \Dcorr~but revealed by \Mgc~.

Running \Mgc~merely requires inputting $n$ samples of two measured properties.  
% Moreover, \Mgc~is the only dependence test that reveals the nature of the observed dependence, therefore providing additional insight that can inform subsequent analysis steps.
Our open source implementation\footnotemark\footnotetext{In both MATLAB and R from our website, \website.} requires essentially the same running  time complexity as conventional methods, situating it to be useful in a wide variety of contexts. 
The following sections document \Mgc's empirical, computational, and theoretical properties. Mathematical details of prior global methods are provided in Appendix \ref{appen:global}, details for \Mgc~are provided in Appendix \ref{appen:mgc}, and \Mgc~pseudocodes are provided in Appendix~\ref{appen:algorithms}.
% @jovo: make sure MGC fills the gap in the below.

\begin{figure}[h!]
  \centering
%  trim={<left> <lower> <right> <upper>}
  \includegraphics[width=1.0\textwidth,trim={3.5cm 0 3.5cm 0cm},clip]{Figures/FigHDPowerMGCM}
  \caption{Power comparison of  \Mgc~to four benchmark dependence tests, for the $20$ different settings.  
Let $\bar{\beta}_s(\mathcal{A})$ denote the average power over a wide range of dimensions for a given problem setting $s$ and algorithm $\mc{A}$. The x-axis shows the difference between the power of \Mgc~and its competitors,  $\bar{\beta}_s(\Mgc)-\bar{\beta}_s(\mathcal{A})$. Power difference $>0$ indicates that \Mgc~achieves higher average power over the benchmark for a given setting;
the large dot on the x-axis indicates the  power differences averaged over all 20 settings.
\Mgc~nearly dominates all benchmarks, exhibiting similar or better power for nearly all settings. 
}
\label{f:nDSummary}
\end{figure}

\subsection*{Finite Sample Simulation Experiments}

When does \Mgc~outperform other approaches, and when does it not?
To answer this question, \Mgc~is compared with four previously proposed state-of-the-art tests: (i) \Mantel, which is widely used in biology and ecology despite a lack of theoretical support \cite{Mantel1967}, (ii) \Dcorr, as discussed above, (iii) \Mcorr, a version of \Dcorr~designed to be unbiased in high-dimensional data \cite{SzekelyRizzo2013a}, and (iv) \Hhg, a powerful test designed for low-dimensional nonlinear settings \cite{HellerGorfine2013}. 
We consider $20$ different noisy dependence settings, largely taken from the existing literature, including  nearly linear (1-5), strongly nonlinear (6-19), and independent (20) settings \cite{SzekelyRizzoBakirov2007, SimonTibshirani2012, GorfineHellerHeller2012, HellerGorfine2013, SzekelyRizzo2013a}.  
Function details are in Appendix~\ref{appen:function}, with additional supporting figures in Appendix~\ref{appen:figs} (the visualization of both noise-free (black) and noisy (gray) samples is in Supplementary Figure~\ref{f:dependencies}).  


% Each approach is evaluated as a function of increasing the dimensionality of $x$.  
Power---the probability of rejecting the null when it is  false---is the standard metric for evaluating test performance of finite samples (see Algorithm~\ref{alg:power} for power computation and Algorithm~\ref{alg:sample_mgc} for \Mgc~test statistic computation).  
For each setting, the power of each test is computed for a large range of different dimensions of $x$,  effectively decreasing the signal-to-noise ratio.  
The average power across dimensions for each algorithm provides a scalar score per setting. Figure~\ref{f:nDSummary} shows the difference between \Mgc's average power and the benchmarks' for each setting.  
\Mgc~achieves a higher power in essentially all 20 settings when compared to all other approaches.  
Supplementary Figure \ref{f:nDAll} shows the power as a function of dimensionality, rather than the average, which indicates that  \Mgc~almost always achieves higher power than the alternative tests for all dimensions, not just the average dimension.  
 Supplementary Figures \ref{f:1DAll} and \ref{f:1DSummary} show similar results,  but keeping the dimensionality of $x$ fixed while increasing sample size. These supplementary figures also show the performance of different variants of \Mgc~with qualitatively similar results.






\subsection*{Revealing the Nature of Dependence}
\label{main3}

\begin{figure}[h!]
\includegraphics[width=1.0\textwidth,trim={3cm 0.5cm 2.3cm 0.5cm},clip]{Figures/FigHDHeat}
\caption{Multiscale Power Maps reveal the influence of neighborhood size on \Mgc~testing power.
For each of the 20 panels, the abscissa and ordinate denote the number of neighbors for $X$ and  $Y$, respectively, and the color denotes the power at that scale. For each simulation, the sample size is $100$,  and the dimension is determined by the largest dimension for \Mgc~to have power exceeding $0.5$ at significance level $0.05$. Each simulation yields a different multiscale power map, and the global scale is optimal only for nearly linear dependencies, highlighting that \Mgc~not only detects the mere existence of dependency, but also reveals its  structure. 
For each panel, the green dot and rectangle show the scale with maximum test statistic,  and the estimated optimal scales. Note that the estimated optimal scales tend to be near the most powerful scales.
}
\label{f:powermaps}
\end{figure}


Not only does \Mgc~provide excellent power, but it also reveals the optimal scales of dependence, which characterize the nature of the dependence structure. 
A \emph{multiscale power map} is a heatmap that shows, for a given simulation, the power as a function of the $x$ and $y$ scales.  
Figure~\ref{f:powermaps} provides the multiscale power maps for all 20 different high-dimensional scenarios, illustrating how the power of local generalized correlations changes with  neighborhood size.
For nearly linear dependencies (1-5), the best neighborhood choice always includes the largest scale, i.e., the global one. For all strongly nonlinear dependencies (6-19),  \Mgc~chooses smaller scales for $x$ or $y$. Thus, a global optimal scale implies a nearly linear dependency, otherwise the dependency is strongly nonlinear.
Furthermore, similar dependencies have similar local generalized correlation structures, and thus, similar multiscale power maps. For example, logarithmic (10) and fourth root (11), though very different functions analytically, are qualitatively similar, and yield very similar power maps.
Similarly,  (12) and (13) are trigonometric functions, and they share a narrow range of significant local generalized correlations.
Both circle (16) and ellipse (17), as well as square (14) and diamond (18), are closely related functions, and have similar  power maps. 

Power map generation requires knowledge of the true distribution of the data, which is unavailable for real data.
For real data, \Mgc~computes a multiscale correlation map, from which it computes the maximum local generalized correlation.  These maps are noisy because they utilize noisy samples, rather than the true distribution, to obtain their values.  Nonetheless, they are useful because they provide the maximum local generalized correlation and estimated optimal scales.  The green dots in Figure \ref{f:powermaps} indicate the scale of maximum local generalized correlation for a single trial, and the green boxes indicate the estimated optimal scales from that trial.  In every case the estimated optimal scales are either very close to or exactly the same as the true optimal scales, indicating that \Mgc~can often correctly estimate the true optimal scales in practice.





\subsection*{\Mgc~Theoretically Dominates its Global Counterparts}
\label{s:theory}

``Oracle \Mgc'' is a version of \Mgc~that uses the true distribution of the data to accurately select the optimal local generalized correlation, rather than estimating it from the data (see Appendix~\ref{appen:mgc2} for details). More specifically, Oracle \Mgc~selects the scale that maximizes power, whereas ``Sample'' \Mgc~selects the scale that maximizes the smoothed test statistic (as described above). 
In either case,  \Mgc~can generalize any distance-based dependence test by restricting it to only consider local distances.  Any global test that \Mgc~generalizes is called \Mgc's ``global counterpart''.  The main theoretical result we obtain is as follows:
% 
\begin{thm} \label{t:dominate}
Oracle \Mgc~statistically dominates its global counterparts. Thus, no matter which 
dependence function, dimensionality, and sample size, 
Oracle \Mgc~achieves equal or higher power than its global counterparts for any global correlation.  More precisely, in \emph{linear} settings Oracle \Mgc~achieves the same power as the global test, and in various nonlinear settings, Oracle \Mgc~achieves {higher} power than the global test. Moreover, Algorithm \ref{alg:all_scales} achieves this dominance with merely an additional multiplicative computational cost of $\log n$, rather than an additional $n^2$ that would result from a na\"ive implementation.
\end{thm}

The above result follows immediately from Theorems \ref{t:thm1}, \ref{t:linear}, and \ref{t:non}, which are described in Appendix \ref{appen:theory}.  
% @jovo: mention quadratic thingy
Empirically, Sample \Mgc~performs very closely to Oracle \Mgc~in most simulated settings (see Supplementary Figure \ref{f:nDAll} and \ref{f:1DAll}), suggesting that Sample \Mgc~may also dominate global methods with high probability.

\subsection*{Real Data Experiments}
\label{numer3}

\Mgc~can be applied to real data scenarios with complex structures, provided appropriate distance measures are available. We apply \Mgc~to three different scenarios: (i) brain activity versus personality, (ii) brain shape versus depression, and (iii) brain networks versus creativity.  For each comparison, we chose appropriate distances for both kinds of data (see Appendix \ref{appen:real} for details), and ran \Mgc~to obtain both a p-value and a multiscale significance map, akin to the multiscale power maps shown above. 
% \Mgc~reveals a statistically significant relationship for all three, and is the only approach that also provides insight into the nature of the relationship. 
% This deeper insight provides guidance for the subsequent analysis tasks, including predictions and causal analysis. As a final test for \Mgc, synthetic data for which no relationship exists are generated, and \Mgc~does not yield spurious relationships, in contrast to popular parametric methods \cite{EklundKnutsson2012,Eklund2015}. 

% @jovo: stopped here

\begin{figure}[h!]
\includegraphics[width=1.0\textwidth,trim={0 0 1.5cm 0},clip]{Figures/FigReal}
\caption{In real data, \Mgc~discovers the dependence and the optimal scales between various brain and mental properties when they exist, and does not detect dependence when it does not exist.  The left three panels show multiscale p-value maps and their corresponding estimated optimal scales for three different experiments: \textbf{(A)}  brain activity vs. five-factor personality model, \textbf{(B)}  brain shape vs depressive disease, and \textbf{(C)} brain networks vs. creativity. Sample size is $42$, $114$, and $109$, respectively, though the ordinate of these panels only goes as high as the largest possible neighborhood size due to repeated entries.  
For all three, \Mgc~yields a significant p-value and reveals the optimal scales of dependence (green rectangles).
\textbf{(D)} Density estimate for the false positive rates of  \Mgc~on the brain activity versus  independent noise experiments, dots indicate the false positive rate of each experiment. The mean $\pm$ standard deviation is $0.0538 \pm 0.0394$ respectively, demonstrating that \Mgc~is a valid test and does not inflate the false positives for these real data.}
\label{f:real}
\end{figure}

\begin{table*}[h!]
\centering
\caption{The p-values for real data testing. \Mgc~is the only method that \emph{always} uncovers the existence of significant relationships and the only method that \emph{ever} discovers the underlying optimal scales. Bold indicates lowest p-value per dataset.}
\label{t:real}%
\begin{tabular}{|c||c|c|c|c|c|}
\hline
Testing Pairs / Methods & Sample \Mgc & \Mantel & \Dcorr & \Mcorr & \Hhg \\
\hline
Activity vs Personality & $\textbf{0.033}$  & $0.988$ & $0.647$ & $0.446$ & $0.056$ \\
\hline
Shape vs Disease & $\textbf{0.019}$  & $0.079$ & $0.108$ & $0.106$ & $0.179$ \\
\hline
Network vs Creativity & $\textbf{0.011}$  & ${0.012}$ & $\textbf{0.011}$ & $\textbf{0.011}$ & ${0.033}$ \\
\hline
\end{tabular}
\end{table*}


% @jovo: in each section, mention the nature of the functions.  and, mention it in the previous experimental sections as well.
\subsubsection*{Brain Activity vs. Personality} 

This experiment investigates whether there is any dependency between resting brain activity and personality.
Adelstein et al. \cite{AdelsteinEtAl2011} were able to detect dependence between the activity of certain brain regions and dimensions of personality, but lacked the tools to test for dependence of whole brain activity against all five dimensions of personality. 
% @jovo: there are nobody believers of 5 dimensions of personality.  bring NEO in earlier.  prob the same for creativity.
This dataset consists of $42$ subjects, each with  $197$ time-steps of resting-state functional MRI activity, as well as the subject's five-factor personality trait as quantified by  the NEO Personality Inventory-Revised  \cite{Costa1992}. 
% 
Figure \ref{f:real}{\color{magenta}A}  shows that many local scales yield significant p-values ($\approx 0.01$), whereas the global scale fails to detect this significant dependence. In fact, all previously proposed global dependence tests under consideration (\Mantel, \Dcorr, \Mcorr, or \Hhg) fail to detect dependence at a significance level of $0.05$ (see Table \ref{t:real}), and only \Mgc~provides insight into the scales of dependence.

\subsubsection*{Brain Shape vs. Depression} 

The next experiment investigates whether there is any dependency between brain shape and depression.  Previous investigations have linked major depressive disorder to hippocampus shape \cite{ParkEtAl2008,PosenerEtAl2003}, though global tests were unable to detect a statistically significant dependence structure at the $\alpha=0.05$ level.
This brain shape versus depression dataset consists of $114$ subjects. Each subject has a structural MRI scan as well as a discrete variable indicating whether the subject is non-affected $(0)$, high-risk $(1)$, or clinically depressed $(2)$.  
% 
Figure \ref{f:real}{\color{magenta}B} provides the p-value map for testing whether the shape of the hippocampus in the right hemisphere is independent of disease status using \Mgc. Again, many local scales yield significant p-values, whereas none of the global methods detect a significant dependence  (see Table \ref{t:real}). 



\subsubsection*{Brain Network vs. Creativity}

The next experiment investigates whether there is any dependency between brain structural networks and creativity.  Neural correlates of creativity have previously been investigated, though largely using structural MRI and cortical thickness \cite{Jung2009}.  Previously published results explored the relationship between graphs and  creativity \cite{Koutra15a}. Those results used   $109$ subjects, each with diffusion weighted MRI data as well as the subject's ``creativity composite index'' (CCI).  
Figure \ref{f:real}{\color{magenta}C} shows, in this case,  that even global dependence tests can ascertain whether the whole brain network is independent of the subject's creativity.  \Mgc~demonstrates that the signal in this case is largely captured by a global relationship, suggesting that there is relatively little to gain by pursuing nonlinear regression techniques. This setting demonstrates that for a small number of high-dimensional structured data samples, \Mgc~can easily reveal a strongly close-to-linear dependence without having to resort to structured regression techniques.


\subsubsection*{\Mgc~Does Not Inflate False Positive Rates} 

% @jovo: maybe start this paragraph with multiple comparisons, there are lots of false positives.  we want to ensure that our thing doesn't do that, so we compared to noise. boom.
In the final experiment, \Mgc~was applied to test whether there is any dependency between brain voxel activities and random numbers, similar to a pair of studies led by Eklund  \cite{EklundKnutsson2012,Eklund2015}. We considered $25$ resting state fMRI data sets from the $1$,$000$ Functional Connectomes Project (\url{http://fcon_1000.projects.nitrc.org/}), consisting of a total of $1$,$583$ subjects.
Then independent random numbers are generated by sampling from a standard normal distribution at each time step. The brain activity data and the random numbers are independent by construction.
For each brain region, \Mgc~attempts to address the following question: Is activity of that  brain region independent of the time-varying stimuli? We pool brain activity over all of the samples from the population.
Any region that is detected as significant is a false positive by definition.  By testing each brain region separately, \Mgc~provides a distribution of false positive rates.  If \Mgc~is valid, that distribution should be centered around the significance level, which is set at $0.05$ for this experiment.



For each data set, the above test is carried out for each brain region. 
Figure~\ref{f:real}{\color{magenta}D} shows the false positive rates of  \Mgc~for each dataset, which are centered around the critical level $0.05$, as it should be.
In contrast, many standard parametric methods for fMRI analysis, such as generalized linear models, can significantly increase the false positive rates, depending on the data and pre-processing details \cite{EklundKnutsson2012,Eklund2015}. Moreover, even the proposed solutions to those issues make linearity assumptions, thereby limiting detection to only a small subset of possible dependence functions.
% @jovo: make the previous subsection shorter by factor of 2.

\subsection*{Discussion}
\label{conclu}

% @jovo: perhaps remove the below paragraph
We propose multiscale generalized correlation (\Mgc) to discover the presence and scales of dependence across disparate types of data.
We proved that Oracle \Mgc~dominates global approaches in finite samples.  This proof shows that \Mgc~performs as well as global approaches in linear settings, and better than global approaches in other settings (nonlinear ones). 
% @jovo: make the above more clear, not all nonlinear settings, not just "certain ones"
We further empirically demonstrate via simulations that \Mgc~outperforms global methods regardless of the dimension, sample size, or nonlinearity.  Moreover, \Mgc~provides a map indicating which scales contain the dependence structure. 
In real data experiments, \Mgc~revealed dependence where global methods fail, as well as the nature of those dependencies, and did not falsely detect signals when there were none.

 \Mgc~can be thought of as a regularized or sparsified variant of generalized correlation coefficients.  Regularization is central to high-dimensional and ill-posed problems, where dimensionality is larger than sample size. The connection made here between regularization and dependence testing opens the door towards considering other regularization techniques for correlation-based dependence testing. For example, \Hhg~can be thought of as averaging over all scales, rather than regularizing to only consider the optimal ones. The Reshef approach can be thought of similarly.  Therefore, we suspect that the ideas underlying \Mgc~could be implemented in other statistical testing frameworks.


% @jovo: do they only deserve mention, or maybe we can build on them? yes, the ideas of MGC could be applied more generally, and other ideas could be incorporated.  pull this down perhaps?
Several other approaches to dependence detection deserve further mention. First, kernel-based independence tests  \cite{GrettonEtAl2005, GrettonGyorfi2010, GrettonEtAl2012} are equivalent ``energy statistics'' (such as \Dcorr~and \Mcorr) \cite{SejdinovicEtAl2013, RamdasEtAl2015}. Thus, we may be able to glean further insights by casting \Mgc~within the kernel framework, or applying \Mgc~to those tests. Specifically, more efficient tests using asymptotic null distribution approximations for our multiscale tests are possible.
Second, Reshef et al. \cite{Reshef2011} and Heller et al. \cite{heller2016consistent} have  different starting configurations, and are therefore slightly more difficult to generalize than the energy statistics based tests.  Nonetheless, they can perform well especially in some $1$-dimensional settings \cite{SimonTibshirani2012, reshef2015empirical}, so further investigation seems worthwhile. 


There are a number of additional potential extensions of this work. First, theoretical guidance for choosing the optimal scale for finite samples in the absence of training data is desirable. 
In this work, we proved that there exist optimal local scales that improve upon the global scale, and demonstrated that Sample \Mgc~yields tests with power very close to the optimal power,  and with significant p-values and useful scale estimation in the real data examples. More theory connecting Oracle \Mgc~and  Sample \Mgc~will yield further insight.

Second, \Mgc~requires a metric (distance function) for each data type. In this work we selected such metrics using domain knowledge and mitigated the potential inopportune choice of metric via locality tuning.  If the optimal metric could be reasonably selected for a given dataset, this would possibly obviate the need for locality, and further improve power. Szekely et al. investigated different norms and exponents to ascertain the impact of different metrics, but was unable to determine optimality.
Metric learning \cite{xing2003distance} explicitly addresses learning metrics for different exploitation tasks, so perhaps could inform a solution to dependence testing.
In fact, \Mgc~can be thought of as a two-dimensional family of ``metrics'', from which the optimal one is determined in a data adaptive fashion.  Perhaps a different low-dimensional family of metrics could further optimize metric and scale.
% @jovo: perhaps mention guillermo's deep learning = metric learning
% @jovo: brett doesn't understand the above paragraph

Third,  all of the tests described in this manuscript require computational space and time essentially quadratic in sample size. 
% @jovo: brett doesn't understand the above. also, aren't they all quadratic? i guess not. this sentence doesn't quite explain the whole paragraph, compress sentence 1 & 2 into 1 of them. "WHich is a problem that must be overcome to analyze large datasets.
When sample size gets very large, computational burden becomes intractable. Huo and Szekely  recently developed efficient algorithms that are linear in sample size for one-dimensional data \cite{Huo2016}.  Although it is not clear how to extend the Huo and Szekely approach to multidimensional data, a subsampling strategy seems viable. In fact, ongoing work suggests the multiscale power maps are accurate even when subsampling the data points significantly, suggesting that subsampling samples or pairwise comparisons could yield an approximate linear time algorithm without sacrificing much testing power.  One could also implement \Mgc~using a semi-external memory and multicore architecture computing model \cite{Zheng2016},  which could enable use of solid state disk space in a streaming fashion to both speed up computations for big data and enable storing interpoint pairwise comparison matrices larger than main memory.

Finally, the notion of multiscale generalized correlations could be applied to a wide variety of statistical tasks.  
% @jovo: because dccorr is a special case of e-stats, and mgc extends dcorr, there is a family of multiscale energy statistics.
In particular, energy statistics---of which $\Dcorr$ and $\Mcorr$ are special cases---have been applied to many different testing scenarios, including goodness-of-fit  \cite{Szekely2005}, analysis of variance  \cite{Rizzo2010}, conditional dependence  \cite{Szekely2014,Wang2015},   and feature selection \cite{LiZhongZhu2012,Zhong2015}.     
In fact, \Mgc~can also implement a two-sample (or generally the $K$-sample) test \cite{Szekely2004, heller2016consistent}; further comparisons of \Mgc~to standard methods for two-sample testing could be illuminating.
% @jovo: "could be illuminating" is meaningless.
Testing independence between graphs and node attributes \cite{Fosdick2015} is another immediate potential application.  In addition, the use of the \Mgc~intuition for dimensionality reduction, classification, and regression seems promising.
% @jovo: logical structure of this paragraph not so clear.
% @jovo: point of this paragraph is that the idea of mgc could be used to make everything

% jovo: something about other things, like isomap, LLE, etc....


\clearpage
\pagestyle{empty}
\bibliographystyle{Science}
\bibliography{MGCbib}


\section*{Acknowledgment}
% \addcontentsline{toc}{section}{Acknowledgment}
This work was partially supported by the
%
National Security Science and Engineering Faculty Fellowship (NSSEFF),
%
the Johns Hopkins University Human Language Technology Center of Excellence (JHU HLT COE),  the
%
Defense Advanced Research Projects Agency's (DARPA) SIMPLEX program through SPAWAR contract N66001-15-C-4041,
%
the XDATA program of DARPA administered through Air Force Research Laboratory contract FA8750-12-2-0303,
%
the Office of Naval Research contract N00014-12-1-0601,
%
the Air Force Office of Scientific Research contract FA955014-1-0033. The authors thank Dr. Brett Mensh of Optimize Science for acting as our intellectual consigliere, and Dr. Ruth Heller and Dr. Yakir Reshef for insightful suggestions.


\clearpage
\appendix
\setcounter{figure}{0}
\renewcommand{\thealgorithm}{C\arabic{algorithm}}
\renewcommand{\thefigure}{E\arabic{figure}}
\renewcommand{\thesubsection}{\thesection.\Roman{subsection}}
%\renewcommand{\thesubsubsection}{\thesubsection.\Roman{subsubsection}}

\section{Global Methods for Testing Dependence}
\label{appen:global}
To better understand the multiscale generalized correlation, in this section we first formally state the testing scenario, followed by introducing the notion of the generalized correlation coefficient and reviewing four existing dependence tests: the \Mantel~test, distance correlation (\Dcorr), modified distance correlation (\Mcorr), and \Hhg. They are arguably the most popular and well-known statistical tests for dependence, and serve as the benchmarks in this paper. Note that the first three are conventional correlation measures, which can be used for building up local generalized correlations and thus  \Mgc.

\subsection{Testing Independence}

A theoretical investigation of the performance of any dependence test requires formalizing the statistical hypotheses.
 Given pairs of observations $(\mb{x}_{i},\mb{y}_{i}) \in \Real^{D \times D_y}$ for $i=1,\ldots,n$, assume they are independently identically distributed as $(\mbx,\mby) \iid f_{xy}$. If the two random variables \mbx~and \mby~are independent, the joint distribution equals the product of the marginals, i.e., $f_{xy}=f_x f_y$.  The statistical hypotheses for testing independence is as follows:
\begin{align*}
& H_{0}: f_{xy}=f_{x}f_{y},\\
& H_{A}: f_{xy} \neq f_{x}f_{y}.
\end{align*}
Given a test statistic, the testing power equals the probability of rejecting the independence hypothesis (i.e. the null hypothesis) when it is false. A test statistic is consistent if and only if the testing power increases to $1$ as sample size increases to infinity. We would like a test to be consistent against most (if not all) dependencies, e.g., \Dcorr, \Mcorr, and \Hhg~are consistent against all dependencies with finite second moments. % which is almost as good as universal consistent in practice because real data are always bounded.
% * <stefaniejacinto@gmail.com> 2017-01-24T00:52:14.101Z:
% 
% > rejecting the independence hypothesis (i.e. the null hypothesis) when it is false
% @jovo: Edited for clarity
% -@sd
% 
% ^.

Note that $D$ is the dimension for $\mb{x}$'s, $D_y$ is the dimensionality for $\mb{y}$'s. For \Mgc~and all benchmark methods, there is no restriction on the dimensions, i.e., the dimensions can be arbitrarily large, and $D$ is not required to equal $D_y$. The ability to handle data of arbitrary dimension is crucial for modern big data, and it is important to recognize tests that excel for high-dimensional data. There also exist some special methods that only operate on 1-dimensional data, such as \cite{Reshef2011,heller2016consistent,Huo2016}, which are not yet generalizable to multidimensional data and thus not considered in this paper.
% * <stefaniejacinto@gmail.com> 2017-01-24T00:55:59.726Z:
% 
% > generalizable
% @jovo: I know Overleaf is marking this as a misspelling but I think it's ok to say "generalizable" instead of "generalize-able"... it's an accepted word
% -@sd
% 
% ^.

\subsection{Generalized Correlation}
Instead of relying on the sample observations directly, most state-of-the-art dependence tests operate on pairwise comparisons, either similarities (such as kernels) or dissimilarities (such as distances). 
Given pairs of observations $(\mb{x}_{i},\mb{y}_{i}) \in \Real^{D \times D_y}$ for $i=1,\ldots,n$, let $\delta_x$ be the distance function for $\mb{x}$'s and $\delta_y$ for $\mb{y}$'s. One can then compute two $n \times n$ distance matrices $\tilde{A}=\{\tilde{a}_{ij}\}$ and $\tilde{B}=\{\tilde{b}_{ij}\}$, where $\tilde{a}_{ij}=\delta_x(\mb{x}_i,\mb{x}_j)$ and $\tilde{b}_{ij}=\delta_y(\mb{y}_i,\mb{y}_j)$. A common example of the distance function is the Euclidean metric (or $L^{2}$ norm), which serves as the starting point for all methods in this manuscript.

Assuming two zero-mean matrices $A$ and $B$ are transformations of $\tilde{A}$ and $\tilde{B}$ (e.g., $A$ and $B$ can be the centered distance matrices),
then a ``generalized correlation coefficient''  \cite{Spearman1904,KendallBook} is written as:
\begin{equation}
\label{generalCoef}
\GG(X,Y)= \tfrac{1}{z} {\textstyle \sum_{i=1}^n \sum_{j=1}^n a_{ij} b_{ij}},
\end{equation}
where $z$ is proportional to the standard deviations of $A$ and $B$, that is $z=n^2\sigma_a \sigma_b$, and $X=\{\mb{x}_{1},\cdots, \mb{x}_{n}\} \in \Real^{D \times n}$ and $Y=\{\mb{y}_{1},\cdots, \mb{y}_{n}\} \in \Real^{D_y \times n}$ denote the matrices of sample observations.

In words, $\GG$ is the global correlation across \emph{pairwise comparison matrices} $A$ and $B$, rather than the individual data samples. By defining $A$ and $B$ based on different transformations of 
the distance matrices, the \Mantel~coefficient, \Dcorr, and \Mcorr~can all be written in the form of the generalized correlation coefficient. Note that traditional correlations such as the Pearson's correlation and the rank correlation can also be written by a generalized correlation coefficient, where $A$ and $B$ are derived from sample observations rather than distances. The only exception is the \Hhg~test, which cannot be easily be cast into this framework due to its usage of rank information.

A generalized correlation always has the range $[-1,1]$, has expectation $0$ under independence, implies a stronger dependency when the correlation is further away from $0$, and can serve as the global correlation in implementing \Mgc. 

To carry out the hypothesis testing on sample data via a nonparametric test statistic, e.g., a generalized correlation, the permutation test is often the best choice \cite{GoodPermutationBook}, because a p-value can be computed by comparing the correlation of the sample data to the correlation of the permuted sample data. The independence hypothesis is rejected if the p-value is lower than a pre-set type $1$ error level, say $0.05$. Then the power of the test statistic equals the probability of correct rejection.

\subsubsection{The \Mantel~Coefficient}
\label{appen:mantel}
Given the Euclidean distance matrices $\tilde{A}$ and $\tilde{B}$, first set $A=\tilde{A}-\bar{a}$ and $B=\tilde{B}-\bar{b}$, where $\bar{a}=\frac{1}{n(n-1)}\sum_{i,j=1}^{n}(\tilde{a}_{ij})$ and similarly for $\bar{b}$; then set the diagonals of $A$ and $B$ to zeros, i.e., $a_{ii}=b_{ii}=0$ for all $i$.
The \Mantel~coefficient \cite{Mantel1967} follows by inserting $A$ and $B$ into Equation~\ref{generalCoef},
%\begin{equation*}
%\Mantel(X,Y)=\frac{\sum_{i \neq j}^{n}a_{ij}b_{ij}}{\sqrt{\sum_{i \neq j}^{n}a_{ij}^2 \sum_{i \neq j}^{n}b_{ij}^2}}.
%\end{equation*}
and the \Mantel~test is carried out by the permutation test.


Unlike distance correlation and \Hhg, so far the \Mantel~test has no consistency proof against all dependent alternatives, 
but it has been a very popular method in biology and ecology, possibly due to its simplicity and effectiveness. Figures~\ref{f:nDAll} and ~\ref{f:1DAll} indeed show that global \Mantel~is sub-optimal relative to much more recently proposed tests, and appears to be inconsistent for many dependencies. 

\subsubsection{Distance Correlation (\Dcorr)}
\label{appen:dcorr}
Given two distance matrices $\tilde{A}$ and $\tilde{B}$, let $A=H\tilde{A}H$, $B=H\tilde{B}H$, where $H=I_{n}-\frac{J_{n}}{n}$ (the double centering matrix), $I_n$ is the $n \times n$ identity matrix (ones on the diagonal, zeros elsewhere), and $J_n$ is the $n \times n$ matrix of all ones. Then the sample distance correlation follows by inserting the above $A$ and $B$ into Equation~\ref{generalCoef}. For distance correlation, the numerator of Equation~\ref{generalCoef} is named the distance covariance, while $\sigma_a$ and $\sigma_b$ in the denominator are named the distance variances. %is defined by doubly centering the distance matrices:
%\begin{equation*}
%\label{dcovEqu}
%dcov(X,Y)=\frac{1}{n^2}\sum_{i,j=1}^{n}a_{ij}b_{ij}.
%\end{equation*}.
%The sample distance variance is defined as
%\begin{align*}
%dvar(X) &=\frac{1}{n^2}\sum_{i,j=1}^{n}a_{ij}^{2},\quad dvar(Y) =\frac{1}{n^2}\sum_{i,j=1}^{n}b_{ij}^{2},
%\end{align*}
%and the sample distance correlation equals
%\begin{equation*}
%\Dcorr(X,Y)=\frac{dcov(X,Y)}{\sqrt{dvar(X) \cdot dvar(Y)}}.
%\end{equation*}

It is shown in \cite{SzekelyRizzoBakirov2007} that as $n \rightarrow \infty$, the sample distance correlation satisfies $\GG(X,Y) \rightarrow \Dcorr(\mb{x},\mb{y}) \geq 0$, where $\Dcorr(\mb{x},\mb{y})$ denotes the population distance correlation between the underlying random variables $\mb{x}$ and $\mb{y}$. 
The population distance correlation is defined via the characteristic functions of $X$ and $Y$, in a way that it equals zero if and only if $\mb{x}$ and $\mb{y}$ are independent. Thus the sample distance correlation is consistent against all dependencies with finite second moments. The distance covariance, distance variance, and distance correlation are always non-negative; the consistency result holds for a family of metrics not limited to the Euclidean distance \cite{Lyons2013}. %Note that the \Dcorr~here equals the square of distance correlation in \cite{SzekelyRizzoBakirov2007}, but for ease of presentation the square naming is dropped here.

Alternatively, calculating the distance covariance by $A=H\tilde{A}$ and $B=\tilde{B}H$ gives the same statistic for distance covariance, i.e., instead of using doubly centered distance matrices, it is equivalent to singly center one distance matrix by row and the other distance matrix by column, as shown in the next lemma.

\begin{lem}
\label{lem1}
The distance covariance is the same under single centering (i.e., $A=H\tilde{A}$ and $B=\tilde{B}H$) and double centering (i.e., $A=H\tilde{A}H$ and $B=H\tilde{B}H$), where $\tilde{A}$ and $\tilde{B}$ are the Euclidean distance matrices of $X$ and $Y$, and $H$ is the centering matrix. 

Moreover, the p-value (via the permutation test) of global \Dcorr~is the same under single centering and double centering, and so is the testing power.
\end{lem}
\begin{proof}
Let $dcov(X,Y)$ denote the numerator of Equation~\ref{generalCoef}, and $\cdot\T$ denote the matrix transpose. Then $dcov(X,Y)$ can be re-written by matrix traces as follows
% * <stefaniejacinto@gmail.com> 2017-01-26T00:29:23.274Z:
% 
% > as follows
% @jovo: I suggest using a double sigma here too.  Also, for the second to the last line, I think we should still have the H in there in the middle, so I've modified it. Previously, it was:
% =tr((A tilde)^T (B tilde) H)
% Please confirm.
% -@sd
% 
% ^ <cshen6@jhu.edu> 2017-01-31T13:30:04.500Z:
% 
% you are right about the H, it is correct now
% -cshen
% 
% ^.
\begin{align*}
dcov(X,Y) &= \sum_{i,j=1}^{n}a_{ij}b_{ij} \\
 &= tr(A\T \times B) \\
 &= tr(H\tilde{A}\T HH\tilde{B}H) \\
 &= tr(\tilde{A}\T H\tilde{B}H) \\
 &= tr((H\tilde{A})\T \times (\tilde{B}H))
\end{align*}
where the derivation follows by using the circular property of traces and noting that $H$ is symmetric and idempotent. Therefore, single centering and double centering yield the same distance covariance.

Although distance variances may not be the same under the two different centering schemes, in the permutation test, the distance variances are merely normalization scalars that do not affect the p-value and power, i.e., the test using distance covariance is the same as the test using distance correlation in the permutation test. Therefore the p-value and power of \Dcorr~are also the same under single centering and double centering.
\end{proof}

\subsubsection{Modified Distance Correlation (\Mcorr)}
\label{appen:mcorr}
In case of high-dimensional data where the dimension $D$ or $D_y$ increases with the sample size $n$, the sample distance correlation may no longer be appropriate \cite{SzekelyRizzo2013a}. For example, even for independent Gaussian distributions, the original distance correlation can converge to $1$ as $D, D_y \rightarrow \infty$. This is because \Dcorr~is a biased statistic at large $D$, which not only makes the interpretation of distance correlation more difficult, but also may impair the testing power of \Dcorr~for high-dimensional data of limited sample size.

Szekely and  Rizzo \cite{SzekelyRizzo2013a, SzekelyRizzo2014, RizzoSzekely2016} therefore proposed the modified/unbiased distance correlation  to eliminate the bias of original \Dcorr. In this paper, we use the following definition for \Mcorr: first let $A'=H\tilde{A}H$ and $B'=H\tilde{B}H$ (i.e., the transformations by original dcorr), then let 
\[a_{ij} = \left\{
  \begin{array}{lr}
    a'_{ij}-\frac{\tilde{a}_{ij}}{n}, & \mbox{ if } i \neq j, \\
    0, &\mbox{ if } i = j,
  \end{array}
\right.
\]
and similarly define $B$. Then \Mcorr~follows by using the above $A$ and $B$ in Equation~\ref{generalCoef}.

It is shown in \cite{SzekelyRizzo2013a} that $\Mcorr(X,Y)$ is an unbiased estimator of the population distance correlation $\Dcorr(\mb{x},\mb{y})$ for all $D, D_y, n$; and \Mcorr~is approximately normal even if $D,D_y \rightarrow \infty$. Thus it always has zero mean under independence, enjoys the same theoretical consistency as \Dcorr, and may work better than \Dcorr~for high-dimensional dependencies. Note that the \Mcorr~here is slightly different from the \Mcorr~in \cite{SzekelyRizzo2013a} (different diagonals), but it is equivalent asymptotically and has almost the same testing performance in finite-sample.
% * <stefaniejacinto@gmail.com> 2017-01-26T02:37:47.279Z:
% 
% > finite-sample.
% @jovo: The use of a hyphen here is usually only when this phrase is used as an adjective. Suggest: "... in finite samples."
% -@sd
% 
% ^.

Similar to the alternative implementation of \Dcorr, singly centered distance matrices can also be used in $A'$ and $B'$ when defining \Mcorr, without altering the theoretical advantages of the original \Mcorr. Therefore, for computational expediency and simplicity, the single-centered \Mcorr~with zero diagonals are used in the \Mgc~implementation.

\subsubsection{Heller, Heller, \& Gorfine (\Hhg)}
\label{appen:hhg}

The \Hhg~statistic applies Pearson's chi-square test to ranks of distances within each column, and is shown to be better than many global tests including \Dcorr~under common nonlinear dependencies in \cite{GorfineHellerHeller2012, HellerGorfine2013}. Like \Dcorr~and \Mcorr, \Hhg~is distance-based and consistent, but not in the form of the generalized correlation coefficient; 
like \Mgc, it makes use of the rank information, but in a different manner.

Given the Euclidean distance matrices $\tilde{A}=\{\tilde{a}_{ij}\}$ and $\tilde{B}=\{\tilde{b}_{ij}\}$, denote
% * <stefaniejacinto@gmail.com> 2017-01-26T02:48:33.606Z:
% 
% > denote
% @jovo: The I's are indicator functions, right? I think I'm more used to seeing a bold 1 instead of an I. Maybe this is a style preference though?
% -@sd
% 
% ^ 
% ^ <cshen6@jhu.edu> 2017-01-31T13:32:58.568Z:
%
% yes they are indicator functions. Feel free to change it to a better style.
% -cshen
%
% ^.
\begin{align*}
H_{11}(i,j) &= \sum_{q=1,q\neq i,j}^{n}\mb{I}(\tilde{a}_{iq} \leq \tilde{a}_{ij})\mb{I}(\tilde{b}_{iq} \leq \tilde{b}_{ij}) \\
H_{12}(i,j) &= \sum_{q=1,q\neq i,j}^{n}\mb{I}(\tilde{a}_{iq} \leq \tilde{a}_{ij})\mb{I}(\tilde{b}_{iq} > \tilde{b}_{ij}) \\
H_{21}(i,j) &= \sum_{q=1,q\neq i,j}^{n}\mb{I}(\tilde{a}_{iq} > \tilde{a}_{ij})\mb{I}(\tilde{b}_{iq} \leq \tilde{b}_{ij}) \\
H_{22}(i,j) &= \sum_{q=1,q\neq i,j}^{n}\mb{I}(\tilde{a}_{iq} > \tilde{a}_{ij})\mb{I}(\tilde{b}_{iq} > \tilde{b}_{ij}).
\end{align*}
Then the \Hhg~statistic is defined as
\begin{align*}
\Hhg(X,Y) &= \sum_{i=1,j\neq i}^{n} \frac{(n-2)(H_{12}(i,j)H_{21}(i,j)-H_{11}(i,j)H_{22}(i,j))^2}{H_{1 \cdot}(i,j)H_{2 \cdot}(i,j)-H_{\cdot 1}(i,j)H_{\cdot 2}(i,j)},
\end{align*}
where $H_{1 \cdot}=H_{11}+H_{12}$, $H_{2 \cdot}=H_{21}+H_{22}$, $H_{\cdot 1}=H_{11}+H_{21}$, and $H_{\cdot 2}=H_{12}+H_{22}$. \Hhg~is structurally distinct from all previous distance-based correlations, and cannot be expressed by Equation~\ref{generalCoef}.

The \Hhg~statistic is consistent when using the permutation test. In our numerical simulations, \Hhg~has relatively low power when testing against high-dimensional and noisy linear dependencies, but is otherwise more advantageous than all global correlations under many nonlinear dependencies, which makes it a strong competitor in general. 
%$\Hhg$ is invariant not only with respect to rescaling of the distances $\delta_x$ and $\delta_y$, but to general monotone transformations.

\section{Multiscale Generalized Correlation (\Mgc)}
\label{appen:mgc}

\subsection{Local Generalized Correlations}
\label{appen:localCorr}

Let $R(a_{ij})$  be the ``rank'' of $\mb{x}_i$ relative to $\mb{x}_j$, that is, $R(a_{ij})=k$ if $\mb{x}_i$ is the $k^{th}$ closest point (or ``neighbor'') to $\mb{x}_j$, and define $R(b_{ij})$ equivalently for the \mby's. For any neighborhood size $k$ around each $\mb{x}_i$~and any neighborhood size $l$ around each $\mb{y}_i$, we define the local pairwise comparisons:
\begin{equation}
\label{localCoef2}
    \mt{a}_{ij}^k=
    \begin{cases}
      a_{ij}, & \text{if } R(a_{ij}) \leq k, \\    
      0, & \text{otherwise};
    \end{cases} \qquad \qquad
    \mt{b}_{ij}^l=
    \begin{cases}
      b_{ij}, & \text{if } R(b_{ji}) \leq l, \\
% * <stefaniejacinto@gmail.com> 2017-01-26T03:13:25.757Z:
% 
% > R(b_{ji})
% @jovo: Is this really b_ji instead of b_ij? Does it have something to do with b-tilde-l_ij being defined based on ranks within each row (as mentioned down in line 629)? If so, we might want to move that note up here for clarity.
% -@sd
% 
% ^.
      0, & \text{otherwise};
    \end{cases}
\end{equation}
and then let $a^k_{ij}=\mt{a}^k_{ij} - \bar{a}^k$, 
where $\bar{a}^k$ is the mean of $\{\mt{a}_{ij}^{k}\}$, and similarly for $b^l_{ij}$.
The \emph{local} variant of any global generalized correlation coefficient is defined as follows, which effectively excludes large distances:
\begin{equation}
\label{localCoef}
\GG^{kl}(X,Y)=\dfrac{1}{z_{kl}} {\textstyle \sum_{i,j=1}^n a_{ij}^k b_{ij}^l},
\end{equation}
where $z_{kl}=n^2 \sigma_a^k \sigma_b^l$,  with $\sigma_a^k$ and $\sigma_b^{l}$ being the standard deviations for the truncated pairwise comparisons. Thus, $c^{kl}$ is the local generalized correlation at a given scale, and the multiscale correlation map can be constructed by computing all local generalized correlations, which allows the discovery of the optimal correlation.

There are a total of $\max(R(a_{ij})) \times \max(R(b_{ij}))$ local generalized correlations, which equals $n^2$ when there exist no repeating values in either sample data. We use minimal ranks in sorting when ties occur, which guarantees that all local generalized correlations are indexed consecutively; alternatively, one may add a very small amount of white noise to break all ties like in the real data experiment.

For any aforementioned generalized correlation coefficient, its local generalized correlations can be directly implemented as in Equation~\ref{localCoef}, by plugging in the respective $a_{ij}$ and $b_{ij}$ from Equation~\ref{generalCoef}. 

Note that we defined the rank-truncated comparisons differently for $\mt{a}_{ij}^k$ and $\mt{b}_{ij}^l$: $\mt{a}_{ij}^k$ is defined based on ranks within each column, while $\mt{b}_{ij}^l$ is defined based on ranks within each row. By doing so, the ranks are consistent between $\tilde{Z}$ and $Z$ for either $Z=A,B$, and the resulting local generalized correlations are always symmetric even if $A$ and $B$ are not symmetric. Moreover, the local generalized correlations of \Dcorr~and \Mcorr~turn out to be more faithful in excluding far-away observations that exhibit insignificant dependency under single centering, such that we always base \Mgcd~and \Mgcm~on single centering throughout the paper. The next lemma justifies the ranking and centering choice.

\begin{lem}
\label{lem2}
Each local generalized correlation $\GG^{kl}$ is always symmetric regardless of the symmetry of $A$ or $B$. Namely for any $k,l$, 
\begin{align*}
\GG^{kl}(X,Y)=\GG^{lk}(Y,X).
\end{align*}
Furthermore, the column ranks of $\tilde{A}$ are preserved in $A$ under single centering but not double centering; similarly, the row ranks of $\tilde{B}$ are preserved in $B$ under single centering.
\end{lem}
\begin{proof}
For fixed $k,l$, denote $R_{A}$ as the binary matrix such that $R_{A}(i,j)=1$ if $rank(a_{ij}) \leq k$, $R_{A}(i,j)=0$ otherwise. Define $R_{B}$ similarly. Then the rank-truncated pairwise comparisons $\mt{a}_{ij}^k$ and $\mt{b}_{ij}^l$ in Equation~\ref{localCoef2} are the entries of $A \circ R_{A}$ and $B \circ R_{B}\T$ respectively, where $\circ$ denotes the entry-wise product.

By the properties of matrix trace, it follows that the local covariance can be rewritten as
\begin{align*}
z_{kl} \GG^{kl}(X,Y) &= \textstyle \sum_{i,j=1}^n a_{ij}^k b_{ij}^l \\
 &= tr((A \circ R_{A})\T \times (B \circ R_{B}\T)) \\
 &= tr((B \circ R_{B}\T) \times (A \circ R_{A})\T) \\
 &= tr((B\T \circ R_{B})\T \times (A\T \circ R_{A}\T)).
\end{align*}

When both $A$ and $B$ are symmetric, it follows that
\begin{align*}
z_{kl} \GG^{kl}(X,Y) &= tr((B\T \circ R_{B})\T \times (A\T \circ R_{A}\T)) \\
 &= tr((B \circ R_{B})\T \times (A \circ R_{A}\T)) \\
 &= z_{lk} \GG^{lk}(Y,X),
\end{align*}
such that $\GG^{kl}(X,Y)=\GG^{lk}(Y,X)$.

Under single centering, however, $A=H \tilde{A}$ and $B=\tilde{B}H$ are no longer symmetric. Nevertheless, the distance matrices $\tilde{A}$ and $\tilde{B}$ are symmetric, so inserting $A\T=\tilde{A}H$ and $B\T=H\tilde{B}$ into the second and fourth equalities above yields
\begin{align*}
z_{kl} \GG^{kl}(X,Y) &= tr(((H \tilde{A}) \circ R_{A})\T \times ((\tilde{B}H) \circ R_{B}\T)) \\
 &= tr(((H \tilde{B}) \circ R_{B})\T \times ((\tilde{A}H) \circ R_{A}\T)) \\
 &= z_{lk} \GG^{lk}(Y,X),
\end{align*}
so that $\GG^{kl}(X,Y)=\GG^{lk}(Y,X)$ under single centering.

Therefore each local generalized correlation $\GG^{kl}$ is always symmetric for \Dcorr~and \Mcorr,~using either double centering or single centering, and for \Mantel~as well.

As for the rank preservation, the column ranks of the Euclidean distance matrix $\tilde{A}$ are the same as the column ranks of $A=H \tilde{A}$, because $H \tilde{A}$ centers each entry of $\tilde{A}$ by column means, while double centering $H \tilde{A} H$ does not always preserve the original column ranks. Similarly, the row ranks of $\tilde{B}$ are preserved in $B=\tilde{B}H$ but not in $H \tilde{B} H$. Note that the ranks are also preserved in \Mantel.
\end{proof}

\paragraph{Computational Complexity}

Assume $D$ is the maximum feature dimension of the two modalities, then distance computation takes $O(n^2 D)$, and the ranking process takes $O(n^2 \log n)$. Once the distance and ranking are done, computing one local generalized correlation requires $O(n^2)$ (see Algorithm \ref{alg:1scale}). Thus a naive approach to compute all local generalized correlations requires at least $O(n^2 \max\{n^2, D\})$ by going through all possible scales, and it seems that \Mgc~is just another theoretical tool with a formidable computational burden. However, given the distance and ranking information, it turns out the multiscale correlation map can be efficiently constructed in $O(n^2)$ by re-using adjacent smaller local generalized correlations to procedurally build all local generalized correlations (see Algorithm \ref{alg:all_scales}), which has essentially the same running time complexity as the global correlations. Therefore, when including the distance computation and ranking overheads, the MGC statistic is computed in $O(n^2 \max\{\log n,D\})$), which has the same running time as the \Hhg~statistic, while the global correlations like \Dcorr~and \Mcorr~run in $O(n^2D)$.

\subsection{Oracle and Sample \Mgc}
\label{appen:mgc2}
Among all local generalized correlation, the multiscale generalized correlation statistic equals the optimal local generalized correlation. \Mgc~can be thought of as a sparse or regularized variant of a global correlation test, and therefore it faces the same dilemma as all regularized algorithms (including sparse methods, feature selection, and dimension reduction): how to efficiently choose the parameters, i.e., the neighborhood scale. By choosing the optimal scale in a principled fashion, \Mgc~both yields a consistent test and reveals the scales of dependence.  

Oracle \Mgc~selects the scale that maximizes power (the probability of correctly rejecting a false null hypothesis, denoted as $\beta$), which depends on the distribution, sample size, and the type $1$ error level:
\begin{align*}
\{(k^{*},l^{*})\} &=\argmax_{(k,l)}\{\beta(\GG_{kl})\}, \\
\GG^{*} &=\max_{(k,l) \in \{(k^{*},l^{*})\}} \{\GG_{kl}\}. 
\end{align*}
The optimal scales always exist,  are distribution dependent, and are often non-unique. For choosing the optimal local generalized correlation, it suffices to ignore $k=1$ or $l=1$: since $\GG^{1l}=\GG^{k1}=\GG^{11}$, they do not include any neighbor other than each observation itself; they merely count the diagonal terms in the distance matrices, and will not be selected as optimal after all.

Therefore, Oracle \Mgc~chooses the optimal scales by simulating from a known or assumed or estimated distribution at a given sample size, and selects the scales that maximize power. In the process, Oracle \Mgc~also yields the multiscale power map that reveals the scales of dependency (Algorithm \ref{alg:power}). Alternatively, if there exists multiple sets of training data, the optimal scales can be selected via the training data. Then the \Mgc~statistic is the optimal local generalized correlation computed on the testing data. Oracle \Mgc~uses a permutation test to obtain the p-value.

However, in real data testing, often the true distribution is unavailable and hard to estimate, and training data are not available. 
So the power map cannot be utilized to calculate the optimal scales or the optimal test statistic.
Instead,  ``Sample \Mgc''  estimates Oracle \Mgc~using the data, by taking the largest correlation that  spans sufficiently many adjacent local generalized correlations. 
If no such correlation exists, Sample \Mgc~defaults the test statistic to the global correlation (Algorithm \ref{alg:sample_mgc}). 
Sample \Mgc~uses a permutation test to obtain the multiscale p-value map and the p-value. 
% 
Once all local p-values are computed, the optimal scales are estimated by the tightest bounding box in the p-value map that is no larger than the p-value of sample \Mgc~(see Algorithm \ref{alg:pval} for details), which serves as an estimation of all potential optimal scales, i.e., the estimated Sample \Mgc~statistic only has one local scale, but the multiscale power maps in Figure~\ref{f:powermaps} clearly show many scales that are close to optimal.

Because neither the Oracle \Mgc~nor Sample \Mgc~compares multiple statistics, neither suffers from the multiple hypothesis testing problem \cite{Benjamini1995}, and the resulting test is always valid. All simulations and real data experiments in the main paper only use Sample \Mgc, while Oracle \Mgc~is added for comparison and for proofs in Appendix~\ref{appen:figs} whenever the underlying model is known.
% * <stefaniejacinto@gmail.com> 2017-01-26T07:19:08.131Z:
% 
% > for comparison and for proofs
% @jovo: I just moved "for proofs" earlier in the sentence, for better flow
% -@sd
% 
% ^.

\subsubsection*{Sample \Mgc~for Biased Correlations}

Sample \Mgc~algorithm can be thought of as taking the largest correlation after smoothing, where the smoothing step identifies adjacent significant correlations in the multiscale correlation map. This algorithm is tailored for \Mgcm, because \Dcorr~and its local generalized correlations are biased (i.e., the expectations may not be $0$ under independence), such that significant correlations cannot be easily determined by the the magnitude of each local statistic. Therefore, the unbiased-ness of \Mcorr~is essential for easily comparing its local generalized correlations and screening out insignificant local generalized correlations very close to $0$, which in turns allows a fast and valid Sample \Mgc~statistic to be designed for \Mgcm. 



Note that a general sample estimation technique can be designed for \Mgcd~and \Mgcp~as well: instead of estimating an optimal correlation by smoothing the local generalized correlation map, one may instead estimate the optimal p-value by smoothing the p-value map (e.g., choose the smallest p-value that spans sufficiently many adjacency scales), then treat the estimated optimal p-value as a test statistic and run the permutation test again to compute the true p-value. The general estimation technique is immune to the bias of local generalized correlations, and is suggested by Heller et al. (2016) \cite{heller2016consistent}. However, it requires more random permutations, is much slower, and does not offer any more theoretical or numerical advantages in testing. Thus in this paper we stick to the current Sample \Mgc~method for \Mcorr~only.
% * <stefaniejacinto@gmail.com> 2017-01-26T07:26:06.897Z:
% 
% > (e.g., by significant p-values expand along adjacency scales)
% @jovo: I'm having trouble understanding this though I admit I'm not familiar with smoothing maps. Can we rewrite this so it's clearer?
% -@sd
% 
% ^ <cshen6@jhu.edu> 2017-01-31T13:37:01.573Z:
%
% I slight rephrased the sentence. What we mean is that choose the smallest p-value such that it is smaller than all adjacent scale p-values. The same holds for smoothing the correlation, but in a reverse way: find the largest correlation that is smaller than correlations on adjacent scales.
% -cshen
%
% ^.


\subsection{Theorems and Proofs of \Mgc}
\label{appen:theory}

%Let $\G_t$ denote a global generalized correlation coefficient based test statistic. For example, $t$ might indicate \Mantel, \Dcorr, or \Mcorr. And let $\beta_n(\G_t^*)$ denote the power of the corresponding Oracle \Mgc~for $n$ samples; we drop the subscript when considering asymptotic power.
Without loss of generality, all theorems in this section are conditioned on a chosen global test yielding test statistic $\GG$.
Recall from the work of Szekely et al. that \Dcorr~and \Mcorr~are both consistent tests, whenever $f_{xy}$ has finite dimension and bounded variance. We further denote the set of distributions satisfying consistency for the given test by $\mc{F}$.
% Then the consistency of \Mgc~is proved as follows:
\begin{thm}
\label{t:thm1}
$\beta_n(\GG^*) \rightarrow 1$ as $n \to \infty$ for all $f_{xy}$ in $\mc{F}$.
In words, Oracle \Mgc~is consistent against all dependent alternatives for which its global counterpart is consistent. 
\end{thm}
\begin{proof}
Since $\beta_n(\Mgc)=\underset{kl}{\max}\{\beta_n(\GG^{kl})\}$, for any $f_{xy}$ the power of the \Mgc~statistic satisfies
\begin{equation*}
\beta_n(\Mgc) \geq \beta_n(\GG)
\end{equation*}
at any type $1$ error level $\alpha$. So $\beta_n(\Mgc) \rightarrow 1$ if $\beta_n(\GG) \rightarrow 1$.
% 
Therefore $\beta_n(\Mgc) \rightarrow 1$ for all $f_{xy}$ in $\mc{F}$. In particular, \Mgcd~and \Mgcm~are consistent with all alternatives satisfying certain regularity conditions, because \Dcorr~and \Mcorr~are consistent by \cite{SzekelyRizzoBakirov2007, SzekelyRizzo2013a}. 
\end{proof}

For finite samples, the distinction between linear or nonlinear dependencies is important for testing and prediction purposes.
For linear dependencies,  the optimal \Mgc~scale was empirically always the global one (recall Figures~\ref{f:powermaps} and \ref{f:powermaps1}). We therefore conjectured and proved the following:
\begin{thm}
\label{t:linear}
If $\mb{x}$ is linearly dependent on $\mb{y}$, then for any $n$ it always holds that
\begin{equation}
\beta_n(\GG^*) = \beta_n(\GG).
\end{equation}
In words, the global scale is the optimal scale for Oracle \Mgc~for linearly dependent data.
\end{thm}
\begin{proof}
To show that the \Mgc~statistic is equivalent to the global correlation coefficient under linear dependence, it suffices to show the p-value of $\GG^{kl}$ is always no less than the p-value of $\GG$ for all $k,l$ and any $n$ under linear dependence. In the permutation test, the p-value equals the percentage of permutations such that the permuted test statistic is no less than the observed test statistic, so it suffices to compare the number of ``significant'' permutations for $\GG$ and $\GG^{kl}$.

Without loss of generality, all of $a_{ij}$, $b_{ij}$, $a_{ij}^{k}$, and $b_{ij}^{l}$ are assumed to have zero mean, because simple centering or not does not affect the p-value. We assume \Dcorr~with double centering is used, as Lemma~\ref{lem1} shows that double centering and simple centering yield the same testing power and p-value.

Under linear dependency, by Cauchy-Schwarz inequality, the distance correlation satisfies
\begin{align*}
& dcov(X,Y) = \sqrt{dvar(X) \cdot dvar(Y)} \quad\Rightarrow\quad 1=\GG(X, Y) \geq \GG(X, Y_{\pi})
\end{align*}
for any permutation $\pi$, where the equality holds if and only if $X$ is a scalar multiple of $Y_{\pi}$, i.e., $a_{ij}=b_{\pi^{-1}(i) \pi^{-1}(j)}$ for all $i,j$, where $\pi^{-1}(\cdot)$ denotes the inverse permutation. 

Thus for the global correlation, there only exist permutations such that the permuted test statistic equals the observed test statistic. However, for all those ``significant'' permutations for $\GG$, they are also ``significant'' for each $\GG^{kl}$, i.e., $a_{ij}^{k}=b_{\pi^{-1}(i) \pi^{-1}(j)}^{l}$ if $a_{ij}=b_{\pi^{-1}(i) \pi^{-1}(j)}$, such that $\GG^{kl}(X, Y)=\GG^{kl}(X, Y_{\pi})$; and there may exist other ``significant'' permutations such that $\GG^{kl}(X, Y) \leq \GG^{kl}(X, Y_{\pi})$.

Therefore the number of ``significant'' permutations for $\GG^{kl}$ at least equals those for $\GG$ under linear dependency, and the p-value of $\GG^{kl}$ cannot be less than the p-value of $\GG$, in which case the global correlation is optimal for \Mgc. 
% * <stefaniejacinto@gmail.com> 2017-01-26T22:04:39.530Z:
% 
% > the p-value of $\GG^{kl}$ cannot be less than the p-value of $\GG$
% @jovo: Can it be equal to the p-value of c?
% -@sd
% 
% ^ <cshen6@jhu.edu> 2017-01-31T13:37:42.324Z:
%
% can be equal.
% -cshen
%
% ^.
\end{proof}

Under nonlinear dependencies and finite sample sizes, empirically \Mgc~achieves better power than its corresponding global correlation. 
We therefore conjectured and proved the following:
\begin{thm}
\label{t:non}
There exists $f_{xy}$ and $n$ such that
\begin{equation}
\beta_n(\GG^*) \geq \beta_n(\GG^{k,l}) > \beta_n(\GG).
\end{equation}
In words, for finite samples, \Mgc~statistic can be better than the global correlation under certain nonlinear dependencies.
\end{thm}
\begin{proof}
We give a simple discrete example of $f_{xy}$ at $n=7$, such that the p-value of \Mgcm~is strictly lower than the p-value of \Mcorr.

Suppose under the alternative, each pair of observations $(\mb{x},\mb{y})$ is sampled as follows:
\begin{align*}
\mb{x} &\in \left\{-1,-\frac{2}{3},-\frac{1}{3},0,\frac{1}{3},\frac{2}{3},1\right\} \mbox{ without replacement}, \\
\mb{y} &= \mb{x}^2,
\end{align*}
which is a discrete version of the quadratic relationship in the simulations.

At $n=7$, $\GG^{kl}(X, Y)$ and $\{\GG^{kl}(X, YQ)\}$ for all permutation matrices $Q$ can be directly calculated. It follows that the p-value of \Mcorr~is $\frac{151}{210} \approx 0.72$, while $\GG^{kl}(X, Y)=\frac{29}{126} \approx 0.23$ at $(k,l)=(2,4)$. Note that in this case, $k$ is bounded above by $n=7$ while $l$ is bounded above by $4$ due to the repeating points in $Y$. 

Then by choosing $\alpha = 0.24$, \Mgc~has power $1$ while global \Mcorr~has power $0$, i.e., \Mgc~successfully identifies the dependency in this example while global \Mcorr~fails.

Note that we can always consider sample points in $[-1,1]$ for $X$, increase $n$, and reach the same conclusion with more significant p-values. However, the computation of all possible permuted test statistics becomes more time-consuming as $n$ increases. The same conclusion also holds for \Mgcd~and \Mgcp~using the same example.
\end{proof}


% The proof of Theorem~\ref{t:linear} is straightforward.  The proof of Theorem~\ref{t:non} is a constructive one, where a quadratic function is constructed and sampled a finite number of times, followed by computing the exact p-value for both \Mgc~and \Dcorr~and proving that \Mgc~has higher power in this setting. This shows that \Mgc~can outperform its global counterpart even for the most modestly nonlinear functions.  
Because any function can be approximated by a polynomial expansion \cite{RudinBook}, the proof of Theorem~\ref{t:non} suggests that \Mgc~is able to outperform its corresponding global correlation on a wide variety of nonlinear functions, which is indeed the case throughout the numerical simulations. 

Taken together, the three theorems above lead to Theorem~\ref{t:dominate} in the main paper.


\clearpage

\section{\Mgc~Algorithms and Testing Procedures}
\label{appen:algorithms}


Six algorithms are presented in order:
\begin{enumerate}
\item Algorithm~\ref{alg:mgc} shows the main steps to utilize \Mgc~on sample observations. It outputs the Sample \Mgc~test statistic and its scale, the p-value via the permutation test, the multiscale maps, and the estimated optimal scales.
\item Algorithm~\ref{alg:power} computes the testing powers for both Sample and Oracle \Mgc~assuming a known model. It also yields the multiscale power map, i.e., the power for each local generalized correlation.
\item Algorithm~\ref{alg:sample_mgc} computes the Sample \Mgc~test statistic. It estimates the optimal local generalized correlation by smoothing the multiscale correlation map (i.e., the largest correlation that spans sufficiently many adjacent scales), thereby estimating Oracle \Mgc.
\item Algorithm~\ref{alg:pval} computes the p-value of Sample \Mgc~by the permutation test, and estimates the optimal scales via the p-value. It also yields the multiscale p-value map. 
\item Algorithm~\ref{alg:1scale} computes the local generalized correlation coefficient at a given scale $(k,l)$, for a given choice of the global correlation coefficient.
\item Algorithm~\ref{alg:all_scales} provides an efficient algorithm to compute all local generalized correlations simultaneously, in the same running time complexity as computing one local generalized correlation. 
\end{enumerate}
For ease of presentation, we assume there are no repeating observations of \mbx~or \mby, and assume \Mcorr~is the global correlation to implement \Mgc.


\clearpage 
\begin{algorithm}
\caption{Multiscale Generalized Correlation (\Mgc). Assuming the maximum feature dimension of $x_i$ and $y_i$ is $D$, the whole \Mgc~algorithm runs in $O(n^2 \times \max(r\log{n}, D))$.}
% * <stefaniejacinto@gmail.com> 2017-01-26T22:23:13.982Z:
% 
% > $O(n^2 \times \max(r\log{n}, D))$.}
% @jovo: This is different from what is declared in line 664 (Computational Complexity, part B.I) which doesn't have the r.  Should we make them consistent?
% -@sd
% 
% ^ <cshen6@jhu.edu> 2017-01-31T13:40:13.780Z:
%
% Computing the MGC stat itself does not need the r, thus line 664.
% The algorithm here is multiplied by r, because we try to further derive the p-value by r MC runs.
% To clarify, in 664 I say the running time is for MGC statistic (rather than MGC algorithm here).
% -cshen
%
% ^.
\label{alg:mgc}
\begin{algorithmic}%[1]
\Require $n$ samples of $(x_i,y_i)$ pairs, an integer $r$ for the number of random permutations.
\Ensure The estimated MGC statistic $\hat{\GG}^*$ and its scale $s$, the p-value $p(\hat{\GG}^*)$, the estimated optimal scales $\mathcal{S}$, the multiscale correlation map $\{c^{kl}\}$ and the multiscale p-value map $P$.
\Function{MGC}{$(x_i,y_i)$, for $i \in [n]$}
\Statex{\textbf{(1)} Calculate all pairwise distances}
\For{$i,j:=1,\ldots,n$}
\State  $a_{ij} = \delta_x(x_i,x_j)$ 
\Comment{$\delta_x$ is the distance between pairs of $x$ samples}
\State  $b_{ij} = \delta_y(y_i,y_j)$
\Comment{$\delta_y$ is the distance between pairs of $y$ samples}
\EndFor
\State Let $A=\{ a_{ij}\}$ and $B=\{ b_{ij}\}$.
% 
\Statex{\textbf{(2)} Calculate Multiscale Correlation Map \& Sample \Mgc~Test Statistic}
% \For{$k,l:=1,\ldots,n$} 
\State  $\{\GG^{kl}\}=\textsc{MGCLocalCorr}(A,B)$  \Comment{local  correlation for all scales using Algorithm \ref{alg:all_scales}}
% $ = \frac{1}{z_{kl}} \sum_{ij} a^k_{ij} b^l_{ij}$
% \EndFor
% \State Let $C=\{ \G^{kl}\}$.
\State Find $[\hat{\GG}^*,s]=\textsc{MGCSampleStatistic}(\{ \GG^{kl}\})$ 
\Comment{estimate optimal statistic using Algorithm \ref{alg:sample_mgc}}
% 
\Statex{\textbf{(3)} Calculate the p-value $p(\hat{\GG}^*)$ and estimated optimal scales $\mathcal{S}$ from Sample \Mgc, as well as the multiscale p-value map $P$}
\State $[p(\hat{\GG}^*),\mathcal{S},P]=\textsc{MGCSampleTest}(A,B,r,\{\GG^{kl}\},\hat{\GG}^*)$ 
\Comment{use Algorithm~\ref{alg:pval}}
\EndFunction
\end{algorithmic}
\end{algorithm}

\clearpage

\begin{algorithm}
\caption{Power computation of \Mgc~given a known distribution. It computes the power for both Sample and Oracle \Mgc, as well as the multiscale power map (i.e., testing powers of all local generalized correlations). By repeatedly simulating samples from the joint distribution $f_{xy}$, sample data of size $n$ under the null and the alternative are generated for $r$ Monte-Carlo replicates. Then all local generalized correlations under the null and the alternative hypotheses are computed by Algorithm~\ref{alg:all_scales}. The power of Sample \Mgc~follows by computing the test statistic under the null and the alternative using Algorithm~\ref{alg:sample_mgc}; Oracle \Mgc~directly maximizes the power map, obtainable by computing the testing power at each local generalized correlation. The running time is $O(rn^2 \log n)$. In the simulations we use $r=10$,$000$ MC replicates. %to estimate the optimal scale, and another $r=10$,$000$ MC replicates to estimate the power. 
% * <stefaniejacinto@gmail.com> 2017-01-26T22:36:10.239Z:
% 
% > The running time is $O(rn^2 \log n)$. 
% @jovo: Same comment as earlier---the running time stated in section B.I doesn't include the r
% -@sd
% 
% ^.
% * <stefaniejacinto@gmail.com> 2017-01-26T22:33:06.649Z:
% 
% > By repeatedly simulating samples from the joint distribution
% @jovo: I think saying "from the joint distribution" is better than saying "by the joint distribution" here
% -@sd
% 
% ^.
This algorithm can be similarly adapted to training data, for which the alternative statistic can be computed from the training data while the null statistic can be computed by permutation. Note that power computation for other benchmarks follows from the same algorithm, by plugging in the respective test statistic in the first loop without the optimal scale computation. }
\label{alg:power}
\begin{algorithmic}[1]
\Require A joint distribution $f_{xy}$, the sample size $n$, the number of MC replicates $r$, and the type $1$ error level $\alpha$.
\Ensure The power of Sample \Mgc~$\beta(\hat{\GG}^{*})$, the power of Oracle \Mgc~$\beta(\GG^{*})$, and the power map $\{\beta_{kl}\} \in [0,1]^{n \times n}$.
\Function{MGCPower}{$f_{xy}$, $n$, $r$, $\alpha$}
\For{$s:=1,\ldots,r$}
\Linefor{$i:=[n]$}{$(x^{1}_{i},x^{1}_{i}) \stackrel{iid}{\sim} f_{xy}$, $x^{0}_{i} \stackrel{iid}{\sim} f_{x}$, $x^{0}_{i} \stackrel{iid}{\sim} f_{y}$} 
\For{$i,j:=1,\ldots,n$}
\State $a^{1}_{ij} = \delta_x(x^{1}_i,x^{1}_j)$ \Comment{pairwise distance under the alternative}
\State $b^{1}_{ij} = \delta_y(y^{1}_i,y^{1}_j)$
\State $a^{0}_{ij} = \delta_x(x^{0}_i,x^{0}_j)$ \Comment{pairwise distance under the null}
\State $b^{0}_{ij} = \delta_y(y^{1}_i,y^{0}_j)$
\EndFor
\State $\{\GG^{kl}_{1}\}[s]=\textsc{MGCLocalCorr}(A^{1},B^{1})$ \Comment{all local generalized correlations under the alternative}
\State $\{\GG^{kl}_{0}\}[s]=\textsc{MGCLocalCorr}(A^{0},B^{0})$ \Comment{all local generalized correlations under the null}
\State $\hat{\GG}^{*}_{1}[s]=\textsc{MGCSampleStatistic}(\{\GG^{kl}_{1}\}[s])$ \Comment{Sample \Mgc~under the alternative}
\State $\hat{\GG}^{*}_{0}[s]=\textsc{MGCSampleStatistic}(\{\GG^{kl}_{0}\}[s])$ \Comment{Sample \Mgc~under the null}
\EndFor

\For{$k,l:=1,\ldots,n$}
\State $\omega_{\alpha} \rto \textsc{Cdf}_{1-\alpha}(\GG_{0}^{kl}[s],s \in [r])$ \Comment{get the critical value by the empirical distributions}
\State $\beta_{kl} \rto \sum_{s=1}^{r}(\GG_{1}^{kl}[s]>\omega_{\alpha}) / r$ \Comment{compute the power map}
\EndFor
\State $\beta(\GG^{*}) \rto \max_{k,l}\{\beta_{kl}\}$  \Comment{testing power of Oracle \Mgc}
\State $\omega_{\alpha} \rto \textsc{Cdf}_{1-\alpha}(\hat{\GG}_{0}^{*}[s],s \in [r])$ 
\State $\beta(\hat{\GG}^{*}) \rto \sum_{s=1}^{r}(\hat{\GG}_{1}^{*}[s]>\omega_{\alpha}) / r$  \Comment{testing power of Sample \Mgc}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Sample \Mgc~test statistic. This algorithm computes the maximum local test statistic, after smoothing, and reports the $(k,l)$ pair that achieves it.  In words, it: (i) finds the largest connected region in the correlation map, such that each correlation is significant, i.e., larger than a certain threshold to avoid correlation inflation by sample noise, (ii) for the largest correlation in the region, calculate the  minimal correlation along adjacent rows and adjacent columns, (iii)  take the larger one as the Sample MGC statistic. If the region area is too small, or the estimated Sample MGC statistic is no larger than the global correlation, use the global correlation instead. The running time is $O(n^2)$.}
\label{alg:sample_mgc}
\begin{algorithmic}[1]
\Require All local statistics $\{\GG^{kl}\} \in \Real^{n \times n}$.
% * <stefaniejacinto@gmail.com> 2017-01-26T22:58:02.986Z:
% 
% > All local statistics
% @jovo: I had a bit more difficulty following this section because of the use of a lot more variables (the Greek letters) that weren't defined in the input section, compared to Algorithm C2 where most of the variables were explicitly defined in Input. Do you think this might benefit from having more detail in the Input section?
% -@sd
% 
% ^ <cshen6@jhu.edu> 2017-01-31T13:45:29.117Z:
%
% I guess this is mostly because the definition of \tau in the algorithm, plus a bunch of additional variables in the for loops? The \tau's can be user-specified parameters, which we can put to input if necessary. But The other variables are auxiliary within the function.
% -cshen
%
% ^.
\Ensure The Sample \Mgc~statistic $\hat{\GG}^{*} \in \Real$, and the corresponding local scale $s=(k,l) \in \mathbb{N} \times \mathbb{N}$.
\Function{MGCSampleStatistic}{$\{\GG^{kl}\}$}
\State $\tau_{1} \rto \sum_{\GG^{kl}<0} (\GG^{kl})^2 / \sum_{\GG^{kl}<0} 1$ \Comment{variance of all negative local generalized correlations}
\State $\tau_{1} \rto \max\{0.01,\sqrt{\tau_1}\} \times 3.5$ \Comment{threshold based on negative correlations}
\State $\tau_{2} \rto \min\{2/n,0.05\}$ \Comment{threshold based on sample size}
\State $R_{kl} \rto \mb{I}(\GG^{kl}>\max\{\tau_{1},\tau_{2}\})$ \Comment{find all correlations that are larger than the thresholds}
\State $R = \textsc{Connected}(R)$ \Comment{largest connected component of all significant correlations}
\State $\hat{\GG}^{*} \rto \GG^{nn}$ \Comment{use the global correlation by default}
\State $s \rto [n,n]$
\If{$\sum_{k,l} R_{kl} \geq n \times \tau_{2} $} \Comment{proceed when the significant region is sufficiently large}
\State $\Omega \rto \{(k,l) :  (\GG^{kl}\geq \max_{R_{kl}=1} \GG^{kl}) \cap (R_{kl}=1)\}$ \Comment{scales with largest correlation in $R$}
\For{$(k',l') \in \Omega$}
\State $[\eta_{1},k] \rto \min_{k \in [k'-\gamma,k'+\gamma]}\{\GG^{kl'}\}$ \Comment{minimal corr and its row index on a fixed column}
\State $\upsilon \rto [k,l']$ 
\State $[\eta_{2},l] \rto \min_{l \in [l'-\gamma,l'+\gamma]}\{\GG^{k'l}\}$ \Comment{minimal corr and its column index on a fixed row}
\State $\eta \rto \max\{\eta_{1},\eta_{2}\}$
\State $\upsilon \rto \upsilon\mb{I}\{\eta_{1} \geq \eta_{2}\}+[k',l]\times \mb{I}\{\eta_{1} < \eta_{2}\}$ \Comment{the local scale of the larger correlation}
\Lineif{$\eta > \hat{\GG}^{*}$}{ $\hat{\GG}^{*} \rto \eta$, $s \rto \upsilon$}
\EndFor
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\clearpage

\begin{algorithm}
\caption{Sample \Mgc~Test. 
This algorithm uses the random permutation test with $r$ random permutations, resulting in the p-value, the estimated optimal scales, and the multiscale p-value map, requiring $O(rn^2 \log n)$. Specifically, it computes the p-values by comparing the multiscale correlation map and the sample \Mgc~statistic of the observed data, to those of each permuted resample.  Then, the optimal scales are estimated by taking the largest rectangle with local p-values no larger than the p-value of Sample \Mgc.  In the real data experiment we always set $r=10$,$000$. Note that the p-value computation for any other global generalized correlation coefficient follows from the same algorithm by replacing Sample \Mgc~with the respective test statistic.
% * <stefaniejacinto@gmail.com> 2017-01-26T23:01:14.101Z:
% 
% > requiring $O(rn^2 \log n)$. 
% @jovo: Same comment as earlier, section B.I didn't have the r
% -@sd
% 
% ^.
}
\label{alg:pval}
\begin{algorithmic}[1]
\Require A pair of distance matrices $(A, B) \in \Real^{n \times n} \times \Real^{n \times n}$, the number of permutations $r$, the local generalized correlation map $\{\GG^{kl}\}$ and sample \Mgc~statistic $\hat{\GG}^*$ for the observed data.
\Ensure The p-value $p \in [0,1]$ for Sample \Mgc, the estimated optimal scale $\mathcal{S} \in \mathbb{N} \times \mathbb{N}$, and the p-value matrix $P \in [0,1]^{n \times n}$ of all local generalized correlations.
\Function{MGCSampleTest}{$A$, $B$, $r$, $\{\GG^{kl}\}$, $\hat{\GG}^*$}
%\State $\{\G^{kl}\}=\textsc{LocalCorr}(A, B)$ \Comment{calculate the observed local generalized correlations}
%\State $\hat{\G}^{*}=\textsc{SampleMGC}(\{\G^{kl}\})$ \Comment{Sample \Mgc~statistic}
\For{$s:=1,\ldots,r$}
\State $\pi=\textsc{RandPerm}(n)$ \Comment{generate a random permutation of size $n$} 
\State $\{\GG^{kl}_{0}\}[s]=\textsc{MGCLocalCorr}(A, B(\pi,\pi))$ \Comment{calculate the permuted local generalized correlations}
\State $\hat{\GG}^{*}_{0}[s]=\textsc{SampleMGC}(\{\GG^{kl}_{0}\}[s])$ \Comment{calculate the permuted Sample \Mgc}
\EndFor

\Linefor{$k,l:=1,\ldots,n$}{$P_{kl} \rto \sum_{s=1}^{r}(\GG^{kl} \leq \GG^{kl}_{0}[s])/r$} \Comment{the p-value map}
\State $p(\hat{\GG}^*) \rto \frac{1}{s}\sum_{s=1}^{r}\mb{I}(\hat{\GG}^{*} \leq \hat{\GG}^{*}_{0}[s])$  \Comment{compute p-value of Sample \Mgc}
\State Construct the binary map:    $\mathcal{E}^{kl} = 1 $ iff $  p(\GG^{kl}) < p(\hat{\GG}^*)$. \Comment{estimate the optimal scales}
\State $\mathcal{S}=$ the set of elements in the largest axis-aligned rectangle in $\mathcal{E}$ containing only $1$'s.

\EndFunction
\end{algorithmic}
\end{algorithm}

\clearpage

\begin{algorithm}
\caption{Compute local test statistic at a given scale. This algorithm runs in $O(n^2)$ once the rank information is provided, which is suitable for \Mgc~computation if an optimal scale is already estimated. But it would take $O(n^4)$ if used to compute all local generalized correlations. Note that for the default \Mgc~implementation by single centering, the centering function centers $A$ by column and $B$ by row, and the sorting function sorts $A$ within column and $B$ within row.}
\label{alg:1scale}
\begin{algorithmic}[1]
\Require A pair of distance matrices $(A, B) \in \Real^{n \times n} \times \Real^{n \times n}$, and the given local scale $(k,l) \in \mathbb{N} \times \mathbb{N}$.
\Ensure The local generalized correlation coefficient $\GG^{kl} \in [-1,1]$ at the given $(k,l)$.
\Function{MGCLocalCorr}{$A$, $B$, $k$, $l$}
\State Initialize $\GG^{kl}$, $V^{A}_{k}$, $V^{B}_{l}$, $E^{A}_{k}$, $E^{B}_{l}$ at $0$.
\Linefor{$Z:=A,B$}{$R^{Z}=\textsc{Sort}(Z)$} \Comment{sort distances}
\Linefor{$Z:=A,B$}{$Z=\textsc{Center}(Z)$}  \Comment{center distance matrices}

\For{$i,j:=1,\ldots,n$}
\State $\GG^{kl} \rto \GG^{kl}+A_{ij}B_{ij}\mb{I}(R^{A}_{ij} \leq k)\mb{I}(R^{B}_{ij} \leq l)$ \Comment{update un-centered local distance covariance}
\Linefor{$Z:=A,B$}{$V^{Z}_{k} \rto V^{Z}_{k}+Z_{ij}^2\mb{I}(R^{Z}_{ij} \leq k)$} \Comment{update local distance variances}
\Linefor{$Z:=A,B$}{$E^{Z}_{k} \rto E^{Z}_{k}+Z_{ij}\mb{I}(R^{Z}_{ij} \leq k)$} \Comment{update sample means}
\EndFor

\State $\GG^{kl} \rto \left(\GG^{kl}-E^{A}_{k}E^{B}_{l}/n^2\right)/\sqrt{\left(V^{A}_{k}-{E^{A}_{k}}^2/n^2\right) \left(V^{B}_{l}-{E^{B}_{l}}^2/n^2\right)}$ \Comment{center and normalize} 

\EndFunction
\end{algorithmic}
\end{algorithm} 

\clearpage

\begin{algorithm}
\caption{Compute the multiscale correlation map (i.e., all local generalized correlations) in $O(n^2 \log n)$. Once the distances are sorted, this algorithm runs in $O(n^2)$. An important observation is that each product $a_{ij}b_{ij}$ is included in $\GG^{kl}$ if and only if $(k,l)$ satisfies $k\leq R(a_{ij})$ and $l\leq R(b_{ij})$, so it suffices to iterate through $a_{ij}b_{ij}$ for $i,j=1,\ldots,n$, and add the product simultaneously to all $\GG^{kl}$ whose scales are no more than $(R(a_{ij}),R(b_{ij}))$. To achieve the above, we iterate through each product, add it to $\GG^{kl}$ at $(k,l)=(R(a_{ij}),R(b_{ij}))$ only (so only one local scale is accessed for each operation); then add up adjacent $\GG^{kl}$ for $k,l=1,\ldots,n$. The same applies to all local covariances, variances, and expectations.} 
\label{alg:all_scales}
\begin{algorithmic}[1]
\Require A pair of distance matrices $(A, B) \in \Real^{n \times n} \times \Real^{n \times n}$.
\Ensure The multiscale correlation map $\{\GG^{kl}\} \in [-1,1]^{n \times n}$ for $k,l=1,\ldots,n$.
\Function{MGCLocalCorr}{$A$, $B$}
\State Initialize $C$ as a zero matrix of size $n \times n$; $V^{A}$, $V^{B}$, $E^{A}$, $E^{B}$ as zero vectors of size $n$.
\Linefor{$Z:=A,B$}{$R^{Z}=\textsc{Sort}(Z)$}
\Linefor{$Z:=A,B$}{$Z=\textsc{Center}(Z)$}

\For{$i,j:=1,\ldots,n$} \Comment{iterate through all local scales to calculate each term} 
\State $k \rto R^{A}_{ij}$
\State $l \rto R^{B}_{ij}$
\State $\GG^{kl} \rto \GG^{kl}+A_{ij}B_{ij}$
\State $V^{A}_{k} \rto V^{A}_{k}+A_{ij}^2$
\State $V^{B}_{l} \rto V^{B}_{l}+B_{ij}^2$
\State $E^{A}_{k} \rto E^{A}_{k}+A_{ij}$
\State $E^{B}_{l} \rto E^{B}_{l}+B_{ij}$
\EndFor

\For{$k:=1,\ldots,n-1$} \Comment{iterate through each scale again and add up adjacent terms} 
\State $\GG^{1, k+1} \rto \GG^{1, k}+\GG^{1, k+1}$
\State $\GG^{k+1,1} \rto \GG^{k+1,1}+\GG^{k+1,1}$
% * <stefaniejacinto@gmail.com> 2017-01-26T23:30:06.653Z:
% 
% > GG^{k+1,1}
% @jovo: Should this be c^{k, 1}  instead of c^{k+1, 1} ?
% -@sd
% 
% ^ <cshen6@jhu.edu> 2017-01-31T13:29:05.743Z:
%
% it is right above. We can change it to c^{k,1} though, if we let k range from 2 to n instead. 
% -cshen
%
% ^.
\Linefor{$Z:=A,B$}{$V^{Z}_{k+1} \rto V^{Z}_{k}+V^{Z}_{k+1}$}
\Linefor{$Z:=A,B$}{$E^{Z}_{k+1} \rto E^{Z}_{k}+E^{Z}_{k+1}$}
\EndFor

\For{$k,l:=1,\ldots,n-1$} 
\State $\GG^{k+1,l+1} \rto \GG^{k+1,l}+\GG^{k,l+1}+\GG^{k+1,l+1}-\GG^{k,l}$
\EndFor

\For{$k,l:=1,\ldots,n$} 
\State $\GG^{kl} \rto \left(\GG^{kl}-E^{A}_{k}E^{B}_{l}/n^2\right)/\sqrt{\left(V^{A}_{k}-{E^{A}_{k}}^2/n^2\right) \left(V^{B}_{l}-{E^{B}_{l}}^2/n^2\right)}$
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

\clearpage

%cs: are we adding the following pseudo code for the real data or not? if so, we need to mention it in main paper real data section and also in the appendix algorithm.
\begin{algorithm}
\caption{% 
constructed the following comparison function. For each scan, 
(i)  run the Configurable Pipeline for the Analysis of Connectomes (C-PAC) pipeline \cite{CPAC2015} to process the raw brain images yielding a parcellation into $197$ regions of interest, 
(ii) run a spectral analysis on each region, (iii) bandpass and normalize it to sum to one, and (iv) calculate the Kullback-Leibler divergence across regions to obtain a similarity matrix comparing all regions.  Then, use the normalized Hellinger distance to compute distances between each subject. 
.} 
\label{alg:real}
\begin{algorithmic}[1]
\Require A pair of distance matrices $(A, B) \in \Real^{n \times n} \times \Real^{n \times n}$.
\Ensure The multiscale correlation map $\{\GG^{kl}\} \in [-1,1]^{n \times n}$ for $k,l=1,\ldots,n$.
\Function{MGCLocalCorr}{$A$, $B$}
\State Initialize $C$ as a zero matrix of size $n \times n$; $V^{A}$, $V^{B}$, $E^{A}$, $E^{B}$ as zero vectors of size $n$.
\Linefor{$Z:=A,B$}{$R^{Z}=\textsc{Sort}(Z)$}
\Linefor{$Z:=A,B$}{$Z=\textsc{Center}(Z)$}

\For{$i,j:=1,\ldots,n$} \Comment{iterate through all local scales to calculate each term} 
\State $k \rto R^{A}_{ij}$
\State $l \rto R^{B}_{ij}$
\State $\GG^{kl} \rto \GG^{kl}+A_{ij}B_{ij}$
\State $V^{A}_{k} \rto V^{A}_{k}+A_{ij}^2$
\State $V^{B}_{l} \rto V^{B}_{l}+B_{ij}^2$
\State $E^{A}_{k} \rto E^{A}_{k}+A_{ij}$
\State $E^{B}_{l} \rto E^{B}_{l}+B_{ij}$
\EndFor

\For{$k:=1,\ldots,n-1$} \Comment{iterate through each scale again and add up adjacent terms} 
\State $\GG^{1, k+1} \rto \GG^{1, k}+\GG^{1, k+1}$
\State $\GG^{k+1,1} \rto \GG^{k+1,1}+\GG^{k+1,1}$
\Linefor{$Z:=A,B$}{$V^{Z}_{k+1} \rto V^{Z}_{k}+V^{Z}_{k+1}$}
\Linefor{$Z:=A,B$}{$E^{Z}_{k+1} \rto E^{Z}_{k}+E^{Z}_{k+1}$}
\EndFor

\For{$k,l:=1,\ldots,n-1$} 
\State $\GG^{k+1,l+1} \rto \GG^{k+1,l}+\GG^{k,l+1}+\GG^{k+1,l+1}-\GG^{k,l}$
\EndFor

\For{$k,l:=1,\ldots,n$} 
\State $\GG^{kl} \rto \left(\GG^{kl}-E^{A}_{k}E^{B}_{l}/n^2\right)/\sqrt{\left(V^{A}_{k}-{E^{A}_{k}}^2/n^2\right) \left(V^{B}_{l}-{E^{B}_{l}}^2/n^2\right)}$
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}




\section{Simulation Dependence Functions}
\label{appen:function}

This section provides the $20$ different dependency functions used in the simulations.  We used essentially the exact same settings as previous publications to ensure a fair comparison \cite{SzekelyRizzoBakirov2007, SimonTibshirani2012, SimonTibshirani2012, GorfineHellerHeller2012}.  We only made changes to add white noise and a weight vector for higher dimensions, thereby making them more difficult, to better compare all methods throughout different dimensions and sample sizes. A few additional settings are also included.

For each sample $\mb{x} \in \Real^{D}$, we denote $\mb{x}_{[d]}, d=1,\ldots,D$ as the $d^{th}$ dimension of the vector \mbx. For the purpose of high-dimensional simulations, $w \in \Real^{D}$ is a decaying vector with $w_{[d]}=1/d$ for each $d$, such that $w\T \mb{x}$ is a 
% one-dimensional 
weighted summation of all dimensions of \mbx. %, which equals \mb{x}~if $D=1$.
Furthermore, $\mc{U}(a,b)$ denotes the uniform distribution on the interval $(a,b)$, $\mc{B}(p)$ denotes the Bernoulli distribution with probability $p$, $\mc{N}(\mu,{\Sigma})$ denotes the normal distribution with mean ${\mu}$ and covariance ${\Sigma}$, 
$u$ and $v$ represent realizations from some auxiliary random variables, $c$ is a scalar constant to control the noise level (which equals $1$ for one-dimensional simulations and $0$ otherwise), and $\epsilon$ is sampled from an independent standard normal distribution unless mentioned otherwise.

For all of the below equations, $(\mb{x},\mb{y}) \overset{iid}{\sim} f_{xy} = f_{y|x} f_x$. For each setting, we provide the space of $(\mb{x},\mb{y})$, and define $f_{y|x}$ and $f_x$, as well as any additional auxiliary distributions.

\setcounter{equation}{0}
\begin{compactenum}
\item Linear $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$,
\begin{align*}
\mb{x} &\sim \mc{U}(-1,1)^{D},\\
\mb{y} &=w\T \mb{x}+c\epsilon.
\end{align*}
\item Exponential $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$:
\begin{align*}
\mb{x} &\sim \mc{U}(0,3)^{D}, \\
\mb{y} &=exp(w\T \mb{x})+10c\epsilon.
\end{align*}
\item Cubic $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$:
\begin{align*}
\mb{x} &\sim \mc{U}(-1,1)^{D}, \\
\mb{y} &=128(w\T \mb{x}-\tfrac{1}{3})^3+48(w\T \mb{x}-\tfrac{1}{3})^2-12(w\T \mb{x}-\tfrac{1}{3})+80c\epsilon.
\end{align*}
\item Joint normal $(\mb{x},\mb{y}) \in \Real^{D} \times \Real^{D}$: Let $\rho=1/2D$, $I_{D}$ be the identity matrix of size $D \times D$, $J_{D}$ be the matrix of ones of size $D \times D$, and $\Sigma = \begin{bmatrix} I_{D}&\rho J_{D}\\ \rho J_{D}& (1+0.5c) I_{D} \end{bmatrix}$. Then
\begin{align*}
(\mb{x}, \mb{y}) &\sim \mc{N}(0, \Sigma). 
\end{align*}
\item Step Function $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$:
\begin{align*}
\mb{x} &\sim \mc{U}(-1,1)^{D},\\
\mb{y} &=\mb{I}(w\T \mb{x}>0)+\epsilon,
\end{align*}
where $\mb{I}$ is the indicator function, that is $\mb{I}(z)$ is unity whenever $z$ true, and zero otherwise.
\item Quadratic $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$:
\begin{align*}
\mb{x} &\sim \mc{U}(-1,1)^{D},\\
\mb{y}&=(w\T \mb{x})^2+0.5c\epsilon.
\end{align*}
\item W Shape $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$:  $u \sim \mc{U}(-1,1)^{D}$,
\begin{align*}
\mb{x} &\sim \mc{U}(-1,1)^{D},\\
\mb{y}&=4\left[ \left( (w\T \mb{x})^2 - \tfrac{1}{2} \right)^2 + w\T u/500 \right]+0.5c\epsilon.
\end{align*}
\item Spiral $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$: $u \sim \mc{U}(0,5)$, $\epsilon \sim \mc{N}(0, 1)$,
\begin{align*}
\mb{x}_{[d]}&=u \sin(\pi u)  \cos^{d}(\pi u) \mbox{ for $d=1,\ldots,D-1$},\\
\mb{x}_{[D]}&=u \cos^{D}(\pi u),\\
\mb{y}&= u \sin(\pi u) +0.4 D\epsilon.
\end{align*}
\item Uncorrelated Bernoulli $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$: $u \sim \mc{B}(0.5)$, $\epsilon_{1} \sim \mc{N}(0, I_{D})$, $\epsilon_{2} \sim \mc{N}(0, 1)$,
% * <stefaniejacinto@gmail.com> 2017-01-27T00:05:28.036Z:
% 
% > mc{N}(0, I_{D})$
% @jovo: What's the difference between this N(0, I D) and N(0,1)^D in functions 12 and 14?
% -@sd
% 
% ^ <cshen6@jhu.edu> 2017-01-31T13:47:03.864Z:
%
% actually no difference :)
% -cshen
%
% ^.
\begin{align*}
\mb{x} &\sim \mc{B}(0.5)^{D}+0.5\epsilon_{1},\\
\mb{y}&=(2u-1)w\T \mb{x}+0.5\epsilon_{2}.
\end{align*}
\item Logarithmic $(\mb{x},\mb{y}) \in \Real^{D} \times \Real^{D}$: $\epsilon \sim \mc{N}(0, I_{D})$
\begin{align*}
\mb{x} &\sim \mc{N}(0, I_{D}),\\
\mb{y}_{[d]}&=2\log_{2}(\mb{x}_{[d]})+3c\epsilon_{[d]},
\end{align*}
for $d=1,\ldots,D$.
\item Fourth Root $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$:
\begin{align*}
\mb{x} &\sim \mc{U}(-1,1)^{D},\\
\mb{y}&=|w\T \mb{x}|^\frac{1}{4}+\frac{c}{4}\epsilon.
\end{align*}
\item Sine Period $4\pi$ $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$: $u \sim \mc{U}(-1,1)$, $v \sim \mc{N}(0,1)^{D}$, $\theta=4\pi$,
\begin{align*}
\mb{x}_{[d]}&=u+0.02 D v_{[d]} \mbox{ for $d=1,\ldots,D$}, \\
\mb{y}&=\sin ( \theta x )+c\epsilon.
\end{align*}
\item Sine Period $16\pi$ $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$: Same as above except $\theta=16\pi$ and the noise on $\mb{y}$ is changed to $0.5c\epsilon$.
\item Square $(\mb{x},\mb{y}) \in \Real^{D} \times \Real^{D}$: Let $u \sim \mc{U}(-1,1)$, $v \sim \mc{U}(-1,1)$, $\epsilon \sim \mc{N}(0,1)^{D}$, $\theta=-\frac{\pi}{8}$. Then
\begin{align*}
\mb{x}_{[d]}&=u \cos\theta + v \sin\theta + 0.05 D\epsilon_{[d]},\\
\mb{y}_{[d]}&=-u \sin\theta + v \cos\theta,
\end{align*}
for $d=1,\ldots,D$.
\item Two Parabolas $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$: $\epsilon \sim \mc{U}(0,1)$, $u \sim \mc{B}(0.5)$,
\begin{align*}
\mb{x} &\sim \mc{U}(-1,1)^{D},\\
\mb{y}&=\left( (w\T \mb{x})^2  + 2c\epsilon\right) \cdot (u-\tfrac{1}{2}).
\end{align*}
\item Circle $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$: $u \sim \mc{U}(-1,1)^{D}$, $\epsilon \sim \mc{N}(0, I_{D})$, $r=1$,
\begin{align*}
\mb{x}_{[d]}&=r \left(\sin(\pi u_{[d+1]})  \prod_{j=1}^{d} \cos(\pi u_{[j]})+0.4 \epsilon_{[d]}\right) \mbox{ for $d=1,\ldots,D-1$},\\
\mb{x}_{[D]}&=r \left(\prod_{j=1}^{D} \cos(\pi u_{[j]})+0.4 \epsilon_{[D]}\right),\\
\mb{y}&= \sin(\pi u_{[1]}).
\end{align*}
\item Ellipse $(\mb{x},\mb{y}) \in \Real^{D} \times \Real$: Same as above except $r=5$.
\item Diamond $(\mb{x},\mb{y}) \in \Real^{D} \times \Real^{D}$: Same as (8) Square except $\theta=-\frac{\pi}{4}$.
\item Multiplicative Noise $(\mb{x},\mb{y}) \in \Real^{D} \times \Real^{D}$: $u \sim \mc{N}(0, I_{D})$, 
\begin{align*}
\mb{x} &\sim \mc{N}(0, I_{D}),\\
\mb{y}_{[d]}&=u_{[d]}\mb{x}_{[d]},
\end{align*}
for $d=1,\ldots,D$.
\item Multimodal Independence $(\mb{x},\mb{y}) \in \Real^{D} \times \Real^{D}$: Let $u \sim \mc{N}(0,I_{D})$, $v \sim \mc{N}(0,I_{D})$, $u' \sim \mc{B}(0.5)^{D}$, $v' \sim \mc{B}(0.5)^{D}$. Then
\begin{align*}
\mb{x}&=u/3+2u'-1,\\
\mb{y}&=v/3+2v'-1.
\end{align*}
\end{compactenum}

For each distribution, $\mb{x}$ and $\mb{y}$ are dependent except  (20); for some settings (8,14,16-18) they are conditionally independent upon conditioning on the respective auxiliary variables, while for others they are
 ``directly'' dependent. 
% Given $(\mb{x}_{i},\mb{y}_{i})$ pairs for $i=1,\ldots,n$, set $X=\{\mb{x}_{1},\cdots, \mb{x}_{n}\} \in \Real^{D \times n}$ and $Y=\{\mb{y}_{1},\cdots, \mb{y}_{n}\} \in \Real^{D_y \times n}$, where $D_y$ is the dimensionality of \mby. 
A visualization of each dependency with $D=D_y=1$ is shown in Figure~\ref{f:dependencies}.


For the increasing dimension simulation in the main paper, we always set $c=0$ and $n=100$, with $D$ increasing.  For types  $4,10,14,18,19,20$, we let $D_y=D$; otherwise, we let $D_y=1$. 
The decaying vector $w$ is utilized for $D>1$ to make the high-dimensional settings more difficult (otherwise, additional dimensions only add more signal).
For the 1-dimensional simulations, we always set $D=D_y=1$, $c=1$ and $n=100$.

\clearpage




\clearpage
\section{Supplementary Figures}
\label{appen:figs}

\begin{figure}[htbp]
\includegraphics[trim={5cm 1.5cm 4cm 0.5cm},clip, width=1.0\textwidth]{Figures/FigSimVisual}
\caption{Visualization of the $20$ dependencies at $D=D_{y}=1$. For each, $n=100$ points are sampled with noise ($c=1$) to show the actual sample data used for 1-dimensional settings (gray dots). For comparison purposes, $n=1000$ points are sampled without noise ($c=0$) to highlight each underlying dependency (black dots). Note that only black points are plotted for type 19 and 20, as they do not have the noise parameter $c$.
}
\label{f:dependencies}
\end{figure}

\begin{figure}[htbp]
\vspace{-50pt}
\includegraphics[width=1.0\textwidth,trim={0 0 0.75cm 0},clip]{Figures/FigA}
\setlength{\tabcolsep}{10pt} % Default value: 6pt
\begin{tabular}{r r r r}
\multicolumn{1}{l}{{\small \textbf{6. Table}}} & & & \\
$\delta_x$(1,2)   & \hspace{1.5em} \color{magenta}-2.03  & \hspace{3.5em} \color{magenta}-1.93  &  \hspace{3.0em} \color{magenta}-1.93  \\ 
 $\delta_y$(1,2) & \color{magenta}-2.30 & \color{magenta}-2.30 & \color{magenta}-2.30  \\ 
 $\delta_x \times \delta_y$ & \color{green}4.67 & \color{green}4.44 & \color{green}4.44  \\ 
 
\hline

 $\delta_x$(2,3) & \color{green}2.53 & \color{green}2.59 & 0.00  \\ 
 $\delta_y$(2,3) &  \color{magenta}-1.36 & \color{magenta}-2.03 & 0.00  \\ 
 $\delta_x \times \delta_y$ & \color{magenta}-3.43 & \color{magenta}-5.26 & 0.00  \\ 

\hline
 $\sum{\delta_x \times \delta_y}$ & \color{magenta}-502.61   & \color{green}92.95 & \color{green}301.33  \\ 
 test statistic &  \color{magenta}-0.02  & 0.00 & \color{green}0.10  \\  
\end{tabular}

\begin{tikzpicture}[remember picture,overlay]
\node[xshift=-5cm,yshift=-5cm] at (current page.east){%
    \includegraphics[width=0.35\textwidth,trim={1.5cm 0cm 1.8cm 0},clip]{Figures/FigB}};
  %\node[anchor=east,inner sep=0pt] at ($(current page.east)-(10cm,10cm)$) {
   %  \includegraphics[width=0.3\textwidth]{Figures/FigB}
  %};
\end{tikzpicture}
\end{figure}
\clearpage
\captionof{figure}{
Schematic  and table demonstrating the ability of Multiscale Generalized Correlation (\Mgc) to detect dependence in nonlinear settings. 
\textbf{0.} 100 pairs of observations $(x_i,y_i)$ are nonlinearly (spirally) dependent on one another.
% 
\textbf{1.} Choose a metric on $x$ and another on $y$, and compute all pairwise distances (centered by the overall means) for $x$ and $y$ yielding interpoint comparison matrices
 $\tilde{A}$ (top) and $\tilde{B}$ (middle), 
and their element-wise products $\tilde{C}=\tilde{A} \circ \tilde{B}$ (bottom), whose normalized sum is the  \Mantel~statistic \cite{Mantel1967} (bottom row of table).
% 
\textbf{2.} Single centering --- subtract the row-sums from $\tilde{A}$ and column-sums from $\tilde{B}$ to eliminate bias due to individual samples --- yields $A=\{a_{ij}\}$ and $B=\{b_{ij}\}$; the normalized sum of their  element-wise product  $C$ is equivalent to the  \Mcorr~statistic \cite{SzekelyRizzo2013a}.
% 
\textbf{3.} Given a local scale, for example, $k=l=4$ here, yields $A^{k}$, $B^{l}$, and $C^{kl}$.  All these test statistics are normalized sums of the element-wise products. The fact that \Mgc~yields a $C^{kl}$ matrix that is all positive, whereas the others yield $C$ matrices with both positive and negative values, suggest that \Mgc~will correctly report a large test statistic here, resulting in a small p-value.
\textbf{4.} Compute the test statistics (top), power (middle), and p-value (bottom) for all local scales, resulting in multiscale maps that reveal the scales of dependency. Green dots show the scale of estimated test statistic by Sample \Mgc~, and the green box shows the estimated optimal scales via the p-value map.
\textbf{5.} Report the corresponding observed test statistics and p-values, and discover the optimal scales (green rectangle in  p-value map) by Sample \Mgc.  
Whereas \Mcorr, the global test, has very low power (magenta dot in  p-value map) and therefore yields a small statistic and a non-significant p-value ($0.257$),  there are many local scales that achieve nearly perfect power, so both Oracle and Sample \Mgc~($\GG^{*}$ and $\hat{\GG}^{*}$) obtain large test statistics and highly significant p-values ($\approx 0.001$) and reveal the scales of dependency. 
\textbf{6.} Numerical demonstration of how \Mgc~is able to detect dependence even in highly nonlinear and low-sample size settings. The three colored points in the scatter plot indicate the three points considered in this table. 
The global methods fail to detect significant dependence since they consider all pairs, including the non-local ones, which \emph{negatively} impact the degree of dependence estimated.
\Mgc~only considers pairs that are jointly local (such as $(1,2)$), while discarding other pairs (such as $(2,3)$). 
% * <stefaniejacinto@gmail.com> 2017-01-27T00:18:28.823Z:
% 
% > while discarding other pairs (such as $(2,3)$). 
% @jovo: Something is wrong with the layout of this page; it's flowing over the page number and NeuroData logo
% -@sd
% 
% ^.
}
\label{f:schematic}


\begin{figure}[htbp]
\includegraphics[width=1.0\textwidth,trim={0 0.5cm 3.2cm 0},clip]{Figures/FigHDPowerAll}
\caption{Power of different methods for $20$ different dependence settings, estimated by Monte Carlo independence tests (see Algorithm \ref{alg:power} for details). It includes eight different tests: \Mcorr, \Dcorr, and \Mantel~(gray solid, dashed, and dashdot lines, respectively), their corresponding Oracle \Mgc~counterparts, \Mgcm, \Mgcd, \Mgcp~(green with same line styles), Sample \Mgc~applied to \Mcorr~(cyan solid), and \Hhg~(gray dotted line). 
Each panel shows the testing power at significance level $\alpha=0.05$ versus the dimensionality of $\mb{x}$'s, for $n=100$ samples. 
Excluding the independent setting (\#20), for which all methods yield power $0.05$, as they should, Oracle \Mgc~empirically achieves similar or better power than the respective global counterpart. In particular, Sample \Mgc~is very close to Oracle \Mgcm, and overall dominates existing approaches for almost all settings and all dimensions, including \Hhg~\cite{HellerGorfine2013}, another state-of-the-art method. Note that \Mgc~is always plotted ``on top'' of the global variants if there is overlap, therefore, some of the global variants are not always visible from the display.}
\label{f:nDAll}
\end{figure}

\begin{figure}[htbp]
\includegraphics[width=1.0\textwidth,trim={0 0.5cm 3.2cm 0},clip]{Figures/Fig1DPowerAll}
\caption{
The same power plots as in Figure~\ref{f:nDAll}, except the $20$ dependence settings are one-dimensional with noise, and the x-axis is increasing sample size.
Each panel shows the testing power on the abscissa at significance level $\alpha=0.05$, and sample size on the ordinate.
Again, Oracle \Mgc~empirically achieves similar or better power than the previous state-of-the-art approaches for all sample sizes on almost all problems, with Sample \Mgc~being very close to Oracle \Mgc~and overall superior to other benchmarks.}
\label{f:1DAll}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth,trim={3.5cm 0 3.5cm 0},clip]{Figures/Fig1DPowerMGCM}
  \caption{The same summary figure as Figure~\ref{f:nDSummary}, but based on the testing power of the $20$ one-dimensional simulation settings in Figure~\ref{f:1DAll}, averaged over sample size. 
  \Mgc~is again the most superior method, exhibiting mean power slightly over \Hhg~for most settings, and very significant advantages over \Mantel, \Dcorr, \Mcorr~for most nonlinear dependencies.}
\label{f:1DSummary}
\end{figure}


\begin{figure}[htbp]
\includegraphics[width=1.0\textwidth,trim={3cm 0.5cm 2.3cm 0.5cm},clip]{Figures/Fig1DHeat}
\caption{Multiscale Power Maps indicating the influence of neighborhood size on \Mgc~testing power, for the one-dimensional simulations in Figure~\ref{f:1DAll}. For each simulation,  the sample size is $n=60$, and the significance level is $\alpha=0.05$. It has similar behavior and interpretation as the high-dimensional power maps in Figure~\ref{f:powermaps}.}
\label{f:powermaps1}
\end{figure}


\section{Real Data Processing}
\label{appen:real}

\subsection{Brain Activity vs. Personality}

For the brain activity modality, we constructed a connectome-based distance (see Appendix \ref{appen:real} for details).
For the five-factor personality modality, we  used the Euclidean distance.


\subsection{Brain Shape vs Depression}

From the MRI data, previous work extracted both the left and right hippocampi. For the brain shape modality, comparison matrices using a nonlinear landmark matching approach were generated \cite{ParkEtAl2008,BegEtAl2005} (see Appendix \ref{appen:real} for details). For the disorder variable, we added white noise bounded by $0.01$ to each label, then formed the Euclidean distance (the noise is used to break ties amongst the discrete values and make sure only the diagonal entries of the distance matrix are zero).

\subsection{Brain Connectivity vs Creativity}


For the raw brain imaging data, we derived the following comparison function.  
% @jovo: put details below in methods, just simple thingy here.
For each scan we estimated brain networks from diffusion and structural MRI data via  \Migraine, a pipeline for estimating brain networks from diffusion data \cite{GrayRoncal2013}, and then computed distance using a graph-based distance (see Appendix \ref{appen:real} for details). 
We used Euclidean distance to compare CCI values. 
% @jovo: we are surprised here that global scale does better than local scales. did we reveal the nature of?


We compute the distance between the graphs using the semi-parametric graph test statistic \cite{Sussman2013,ShenVogelsteinPriebe2016,Tang2016}, embedding each graph into two dimensions for simplicity and aligning the embeddings via a Procrustes analysis.


\subsection{Brain Activity vs. Noise}

\Mgc~requires a distance matrix for brain region activity, and another for the stimulus. For the brain region activity, we used C-PAC to estimate regional time-series, in particular, using the sequence of pre-processing decisions determined to optimize discriminability \cite{Wang2016}.  The output for each scan is the resting state fMRI time series data containing $200$ regions of interest for $200$ time-steps.
For each region, the Euclidean distance pairs between time steps are computed, i.e., $\|\mb{x}_{\cdot i}-\mb{x}_{\cdot j}\|_2$,  where $\mb{x}_{\cdot i}$ denotes the population vector of activity of the region at time-step $i$ for all subjects.
For the one-dimensional stimulus, we similarly compute the Euclidean distance between the stimulus values at each pair of time-steps: $\|\mb{y}_i - \mb{y}_j\|_2$.
Note that the distance matrices at different brain regions are distinct, but the stimulus is the same for all brain regions during the same experiment.


\end{document}
